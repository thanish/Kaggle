{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'sage'\n",
    "model_name = 'epoch_0_model_sage_Albert_base_uncased.bin'\n",
    "\n",
    "if platform == 'Azure':\n",
    "    bert_path = '/home/thanish/transformer_models/bert_base_uncased'\n",
    "    test_path = '../test/*'\n",
    "    model_path = '../output/'\n",
    "elif platform == 'Kaggle':\n",
    "    bert_path = '../input/bertlargeuncasedpytorch'\n",
    "    test_path = '/kaggle/input/coleridgeinitiative-show-us-the-data/test/*'\n",
    "    model_path = '../input/coleridgemodels/'\n",
    "else:\n",
    "    bert_path = 'C:/Users/thanisb/Documents/transformer_models/bert_base_uncased/'\n",
    "    test_path = '../test/*'\n",
    "    model_path = '../output/'\n",
    "    \n",
    "config = {'MAX_LEN':512,\n",
    "          'tokenizer': AutoTokenizer.from_pretrained('albert-base-v2' , do_lower_case=True),\n",
    "          'batch_size':20,\n",
    "          'Epoch': 3,\n",
    "          'test_path':test_path, \n",
    "          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "          'model_path':model_path,\n",
    "          'model_name':model_name\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_joining(data_dict_id):\n",
    "    '''\n",
    "    This function is to join all the text data from different sections in the json to a single\n",
    "    text file. \n",
    "    '''\n",
    "    data_length = len(data_dict_id)\n",
    "\n",
    "    #     temp = [clean_text(data_dict_id[i]['text']) for i in range(data_length)]\n",
    "    temp = [data_dict_id[i]['text'] for i in range(data_length)]\n",
    "    temp = '. '.join(temp)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_json(test_data_folder):\n",
    "    '''\n",
    "    This function reads all the json input files and return a dictionary containing the id as the key\n",
    "    and all the contents of the json as values\n",
    "    '''\n",
    "\n",
    "    test_text_data = {}\n",
    "    total_files = len(glob.glob(test_data_folder))\n",
    "    \n",
    "    for i, test_json_loc in enumerate(glob.glob(test_data_folder)):\n",
    "        filename = test_json_loc.split(\"/\")[-1][:-5]\n",
    "\n",
    "        with open(test_json_loc, 'r') as f:\n",
    "            test_text_data[filename] = json.load(f)\n",
    "\n",
    "        if (i%1000) == 0:\n",
    "            print(f\"Completed {i}/{total_files}\")\n",
    "\n",
    "    print(\"All files read\")\n",
    "    return test_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0/4\n",
      "All files read\n"
     ]
    }
   ],
   "source": [
    "test_data_dict = read_test_json(test_data_folder=\"../test/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3f316b38-1a24-45a9-8d8c-4e05a42257c6\n",
      "2f392438-e215-4169-bebf-21ac4ff253e1\n",
      "8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60\n",
      "2100032a-7c33-4bff-97ef-690822c43466\n"
     ]
    }
   ],
   "source": [
    "for i in test_data_dict.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the saved model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForTokenClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model checkpoint: ../output/epoch_0_model_sage_Albert_base_uncased.bin\n",
      "Checkpoint loaded\n",
      "Model loaded with all keys matching with the checkpoint\n"
     ]
    }
   ],
   "source": [
    "# initializing the model\n",
    "model = transformers.AlbertForTokenClassification.from_pretrained('albert-base-v2',  num_labels = 3)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# Reading the trained checkpoint model\n",
    "trained_model_name = config['model_path'] + config['model_name']\n",
    "print(\"Trained model checkpoint:\", trained_model_name)\n",
    "checkpoint = torch.load(trained_model_name, map_location = config['device'])\n",
    "print(\"Checkpoint loaded\")\n",
    "\n",
    "# Matching the trained checkpoint model to the initialized model\n",
    "model.load_state_dict(checkpoint)\n",
    "print(\"Model loaded with all keys matching with the checkpoint\")\n",
    "\n",
    "model = model.to(config['device'])\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def prediction_fn(tokenized_sub_sentence):\n",
    "\n",
    "    tkns = tokenized_sub_sentence\n",
    "    indexed_tokens = config['tokenizer'].convert_tokens_to_ids(tkns)\n",
    "    segments_ids = [0] * len(indexed_tokens)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(config['device'])\n",
    "    segments_tensors = torch.tensor([segments_ids]).to(config['device'])\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit = model(tokens_tensor, \n",
    "                      token_type_ids=None,\n",
    "                      attention_mask=segments_tensors)\n",
    "\n",
    "        logit_new = logit[0].argmax(2).detach().cpu().numpy().tolist()\n",
    "        prediction = logit_new[0]\n",
    "\n",
    "#         print(tkns)\n",
    "#         print(logit_new)\n",
    "#         print(prediction)\n",
    "        \n",
    "        kword = ''\n",
    "        kword_list = []\n",
    "\n",
    "        for k, j in enumerate(prediction):\n",
    "            if (len(prediction)>1):\n",
    "\n",
    "                if (j!=0) & (k==0):\n",
    "                    #if it's the first word in the first position\n",
    "                    #print('At begin first word')\n",
    "                    begin = tkns[k]\n",
    "                    kword = begin\n",
    "\n",
    "                elif (j!=0) & (k>=1) & (prediction[k-1]==0):\n",
    "                    #begin word is in the middle of the sentence\n",
    "                    begin = tkns[k]\n",
    "                    previous = tkns[k-1]\n",
    "\n",
    "                    if not begin.startswith('▁'):\n",
    "                        kword = previous + begin[:]\n",
    "                    else:\n",
    "                        kword = begin\n",
    "\n",
    "                    if k == (len(prediction) - 1):\n",
    "                        #print('begin and end word is the last word of the sentence')\n",
    "                        kword_list.append(kword.rstrip().lstrip().replace('▁', ''))\n",
    "\n",
    "                elif (j!=0) & (k>=1) & (prediction[k-1]!=0):\n",
    "                    # intermediate word of the same keyword\n",
    "                    inter = tkns[k]\n",
    "\n",
    "                    if not inter.startswith('▁'):\n",
    "                        kword = kword + \"\" + inter[:]\n",
    "                    else:\n",
    "                        kword = kword + \" \" + inter\n",
    "\n",
    "\n",
    "                    if k == (len(prediction) - 1):\n",
    "                        #print('begin and end')\n",
    "                        kword_list.append(kword.rstrip().lstrip().replace('▁', ''))\n",
    "\n",
    "                elif (j==0) & (k>=1) & (prediction[k-1] !=0):\n",
    "                    # End of a keywords but not end of sentence.\n",
    "                    kword_list.append(kword.rstrip().lstrip().replace('▁', ''))\n",
    "                    kword = ''\n",
    "                    inter = ''\n",
    "            else:\n",
    "                if (j!=0):\n",
    "                    begin = tkns[k]\n",
    "                    kword = begin\n",
    "                    kword_list.append(kword.rstrip().lstrip().replace('▁', ''))\n",
    "#         print(kword_list)\n",
    "#         print(\"\")\n",
    "    return kword_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_sent_split(text):\n",
    "    sent_split = text.split(\" \")\n",
    "\n",
    "    start = 0\n",
    "    end = len(sent_split)\n",
    "    max_length = 64\n",
    "\n",
    "    final_sent_split = []\n",
    "    for i in range(start, end, max_length):\n",
    "        temp = sent_split[i: (i + max_length)]\n",
    "        final_sent_split.append(\" \".join(i for i in temp))\n",
    "    return final_sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data_dict):\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for i, Id in enumerate(data_dict.keys()):\n",
    "        current_id_predictions = []\n",
    "        \n",
    "        print(Id)\n",
    "        sentences = data_joining(data_dict[Id])\n",
    "        sentence_tokens = sent_tokenize(sentences)\n",
    "        \n",
    "        for sub_sentence in sentence_tokens:\n",
    "            cleaned_sub_sentence = clean_text(sub_sentence)\n",
    "        \n",
    "            # Tokenize the sentence\n",
    "            tokenized_sub_sentence = config['tokenizer'].tokenize(cleaned_sub_sentence)\n",
    "            \n",
    "            if len(tokenized_sub_sentence) == 0:\n",
    "                # If the tokenized sentence are empty\n",
    "                sub_sentence_prediction_kword_list = []\n",
    "                \n",
    "            elif len(tokenized_sub_sentence) <= 512:\n",
    "                # If the tokenized sentence are less than 512\n",
    "                sub_sentence_prediction_kword_list = prediction_fn(tokenized_sub_sentence)\n",
    "\n",
    "            else:\n",
    "                # If the tokenized sentence are >512 which is long sentences\n",
    "                long_sent_kword_list = []\n",
    "                \n",
    "                tokenized_sub_sentence_tok_split = long_sent_split(text = tokenized_sub_sentence)\n",
    "                for i, sent_tok in enumerate(tokenized_sub_sentence_tok_split):\n",
    "                    if len(sent) != 0:\n",
    "                        kword_list = prediction_fn(sent_tok)\n",
    "                        long_sent_kword_list.append(kword_list)\n",
    "                flat_long_sent_kword = [item for sublist in long_sent_kword_list for item in sublist]\n",
    "                sub_sentence_prediction_kword_list = flat_long_sent_kword\n",
    "                            \n",
    "            if len(sub_sentence_prediction_kword_list) !=0:\n",
    "                current_id_predictions = current_id_predictions + sub_sentence_prediction_kword_list\n",
    "\n",
    "        results[Id] = list(set(current_id_predictions))\n",
    "                \n",
    "    print(\"All predictions completed\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_few_word_prediction(prediction_dict):\n",
    "    final_result = {}\n",
    "    for ID in prediction_dict.keys():\n",
    "        temp = []\n",
    "\n",
    "        for pred in prediction_dict[ID]:\n",
    "            pred_split = pred.split(\" \")\n",
    "            condition1 = len(pred_split)<=2\n",
    "            condition2 = 'adni' not in pred\n",
    "            condition3 = 'cccsl' not in pred\n",
    "            condition4 = 'ibtracs' not in pred\n",
    "            condition5 = 'slosh model' not in pred\n",
    "            \n",
    "            if condition1 & condition2 & condition3 & condition4 & condition5:\n",
    "                pass\n",
    "            else:\n",
    "                temp.append(pred)\n",
    "        final_result[ID] = temp\n",
    "        \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_1 = get_predictions(data_dict = test_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "81\n",
      "59\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'3f316b38-1a24-45a9-8d8c-4e05a42257c6': ['sea level rise',\n",
       "  'coastal erosion beach',\n",
       "  'slr to natural resources habitats',\n",
       "  'floodplain mapping program',\n",
       "  'coastal erosion study',\n",
       "  'lrr rate of change statistic',\n",
       "  'coastal flood risk study',\n",
       "  'bodie island lifesaving and coast guard stations',\n",
       "  'storm surge inundation mapping',\n",
       "  'oil house structures',\n",
       "  'north carolina flood mapping program',\n",
       "  'community vulnerability and coastal development',\n",
       "  'human natural system and barrier spit',\n",
       "  'shoreline analysis system d',\n",
       "  'potential inundation maps',\n",
       "  'dcm sea level site',\n",
       "  'storm surge wave energy',\n",
       "  'fema digital flood insurance rate',\n",
       "  'other grid products',\n",
       "  'nfip flood risk projects',\n",
       "  'region to storm surge and',\n",
       "  'coastal erosion storm surge and sea level rise',\n",
       "  'nc coastal erosion study',\n",
       "  'gov geoportal catalog',\n",
       "  'north carolina floodplain mapping program',\n",
       "  'coastal ocean waters',\n",
       "  'high historic rate of erosion',\n",
       "  'shoreline and basemap information',\n",
       "  'budget planning efforts',\n",
       "  'interannual change data',\n",
       "  'nc flood risk information system',\n",
       "  'nc sea grant coastal policy fellowship',\n",
       "  'coastal mapping projects',\n",
       "  'baseline topographic and bathymetric data',\n",
       "  'tropical storms and hurricanes',\n",
       "  'bathtub inundation maps',\n",
       "  'storm surge mapping',\n",
       "  'storm surge vulnerability',\n",
       "  'national hydrography dataset',\n",
       "  'kitchen shed and flag house at hatter',\n",
       "  'hurricane preparedness and safety open house',\n",
       "  'sea level trends',\n",
       "  'caha historic resource assets',\n",
       "  'coastal salinity database',\n",
       "  'mom inundation maps',\n",
       "  'north carolina emergency management spatial data',\n",
       "  'driven overwash and longshore',\n",
       "  'lake and overland surges from hurricanes',\n",
       "  'barrier island system',\n",
       "  'a core product',\n",
       "  'shoreline change assessment',\n",
       "  'ocean shoreline erosion',\n",
       "  'hurricane in usgs coastal vulnerability index',\n",
       "  'sea level rise viewer',\n",
       "  'sea level rise mapping project',\n",
       "  'alongshore pulses of erosion',\n",
       "  'fish and wildlife service the north carolina division of transportation',\n",
       "  'shoreline movement ns',\n",
       "  'coastal observation station',\n",
       "  'national flood insurance program nfip',\n",
       "  'inundation and surges',\n",
       "  'national wildlife refuge',\n",
       "  'estuarine shoreline movement',\n",
       "  'new jersey beach profile network',\n",
       "  'lidar elevation data',\n",
       "  'national geodetic survey',\n",
       "  'direct landfalling hurricanes',\n",
       "  'las dataset data',\n",
       "  'beach monitoring program',\n",
       "  'surge from hurricanes',\n",
       "  'aerial imagery of weather bureau located in hatteras',\n",
       "  'slosh display program',\n",
       "  'shoreline change maps',\n",
       "  'coastal erosion storm',\n",
       "  'future sea level rise',\n",
       "  'the slosh display program',\n",
       "  'nc floodplain mapping program',\n",
       "  'coastal flood risk mapping',\n",
       "  'flood depth grid',\n",
       "  'community of hatteras',\n",
       "  'erosion and storm surges',\n",
       "  'sea level rise inundation maps',\n",
       "  'slosh mom surges',\n",
       "  'floodplain boundaries flood zones and base flood elevations',\n",
       "  'current climatology of tropical storms',\n",
       "  'noaa storm surge',\n",
       "  'projections of sea level rise',\n",
       "  'future lidar building and infrastructure data',\n",
       "  'level rise risk management study',\n",
       "  'storm surge community vulnerability',\n",
       "  'slosh inundation grid',\n",
       "  'storm surge impact',\n",
       "  'natural migration of sediment',\n",
       "  'first floor elevation data',\n",
       "  'coastal storm damage',\n",
       "  'geographic information systems',\n",
       "  'shoreline change analysis',\n",
       "  'bodie island life saving site',\n",
       "  'elevated ground water tables',\n",
       "  'caha study area',\n",
       "  'coastal elevation data',\n",
       "  'sediment budget of the shore and its net movement',\n",
       "  'extratropical and tropical storms',\n",
       "  'coastal resources commission',\n",
       "  'slosh model',\n",
       "  'north meters coordinate system',\n",
       "  'nc floodplain mapping',\n",
       "  'coastal change hazards',\n",
       "  'sea level rise of values',\n",
       "  'current sea level',\n",
       "  'coastal management policy device',\n",
       "  'storm surge and sea level rise',\n",
       "  'atlantic ocean figure'],\n",
       " '2f392438-e215-4169-bebf-21ac4ff253e1': ['progress in international reading literacy study',\n",
       "  'school age population',\n",
       "  'education equity of resources',\n",
       "  'mathematics literacy and science literacy',\n",
       "  'pisa data collection effort',\n",
       "  'societal implications of science and technological developments',\n",
       "  'international socioeconomic index of occupational status',\n",
       "  'countries reporting data',\n",
       "  '8 countries reporting data',\n",
       "  'education on income net of occupation',\n",
       "  'national school system',\n",
       "  'national accounts database',\n",
       "  'forestry and fishery',\n",
       "  'mathematics and science',\n",
       "  'on education internationally',\n",
       "  'benchmarks in mathematics and science',\n",
       "  'international education data',\n",
       "  'trends in international mathematics and science study',\n",
       "  'current population survey',\n",
       "  'higher employment rates',\n",
       "  'foreign students in higher education figure',\n",
       "  'common of secondary education gcse students',\n",
       "  'standard classification of education major field of study',\n",
       "  'education expenditure data',\n",
       "  'international standard classification of education',\n",
       "  'foreign students in higher education',\n",
       "  'school questionnaire achievement',\n",
       "  'country reporting data',\n",
       "  'social sciences business and law',\n",
       "  'grade teacher questionnaire',\n",
       "  'integrated postsecondary education data system',\n",
       "  'timss science achievement scales',\n",
       "  'in science literacy',\n",
       "  'education system of germany',\n",
       "  'national education systems ines survey',\n",
       "  'in science mathematics and engineering related fields',\n",
       "  'countries reporting data in',\n",
       "  'organization for economic cooperation and development',\n",
       "  'history geography science art and music',\n",
       "  'national sample surveys',\n",
       "  'commerce management engineering',\n",
       "  'vocational higher education',\n",
       "  'solar system human health and energy',\n",
       "  'science literacy scale',\n",
       "  'school enrollment and population data',\n",
       "  'violating dress code classroom disturbance cheating profanity',\n",
       "  'academic higher education employment rates',\n",
       "  'national purchasing power parities ppps exchange rate data',\n",
       "  'high benchmark in science',\n",
       "  'digest of education statistics',\n",
       "  'benchmark in science',\n",
       "  'pirls international performance benchmark',\n",
       "  'pirls reading achievement scales',\n",
       "  'students in science literacy',\n",
       "  'gdp on education 6',\n",
       "  'common degree programs certificates of higher education',\n",
       "  'program for international student assessment',\n",
       "  'national center for education statistics',\n",
       "  'upper secondary education employment rates',\n",
       "  'national salary schedules',\n",
       "  'ages of universal enrollment',\n",
       "  'upper secondary education 27',\n",
       "  'national education data',\n",
       "  'teacher salary data',\n",
       "  'international standard classification of occupations',\n",
       "  'professional development in science assessment',\n",
       "  'international data base',\n",
       "  'schools and staffing survey',\n",
       "  'levels of gross domestic product gdp per',\n",
       "  'collections school surveys',\n",
       "  'grade science assessment average',\n",
       "  'further education sector colleges',\n",
       "  'secondary school entrance exams',\n",
       "  'data for gdp per capita',\n",
       "  'gdp on education',\n",
       "  'higher education completion in',\n",
       "  'online education database',\n",
       "  'for population and enrollment data',\n",
       "  'participation and response rate standards',\n",
       "  'instructional resource allocation',\n",
       "  'highest benchmark in science advanced ranged'],\n",
       " '8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60': ['manipulation and analysis iri consumer network panel data',\n",
       "  'food retail and multicultural households',\n",
       "  'national sampling weights',\n",
       "  'current retail trends',\n",
       "  'meats like poultry and fish',\n",
       "  'products bread milk',\n",
       "  'intercept data analysis',\n",
       "  'regular ground beef white bread',\n",
       "  'hispanics purchasing seafood and poultry',\n",
       "  'regular ground beef and white bread',\n",
       "  'customer intercept surveys',\n",
       "  'using secondary data',\n",
       "  'wheat bread peach',\n",
       "  'renewable agriculture and food systems',\n",
       "  'beef wheat bread frozen',\n",
       "  'customer intercept survey data',\n",
       "  'federal food assistance programs',\n",
       "  'food at home purchases',\n",
       "  'food access problems',\n",
       "  'intercept survey and ces dataset',\n",
       "  'regular beef white bread',\n",
       "  'our intercept survey analysis',\n",
       "  'iri data analysis',\n",
       "  'nielsen homescan survey',\n",
       "  'on households and race',\n",
       "  'primary and secondary food stores',\n",
       "  'national health and nutrition examination survey national food survey',\n",
       "  'food store types',\n",
       "  'for food shopping',\n",
       "  'our intercept survey',\n",
       "  'regular ground beef',\n",
       "  'food access research',\n",
       "  'packaged meat and juices',\n",
       "  'national bureau of economic research n',\n",
       "  'rural residents shopping',\n",
       "  'poultry and seafood',\n",
       "  'food shopping patterns',\n",
       "  'food retail segment',\n",
       "  'national dataset foodaps',\n",
       "  'traditional supermarkets for their food shopping',\n",
       "  'snap and food insecure households',\n",
       "  'lean ground beef',\n",
       "  'mail and data collection',\n",
       "  'food insecurity rate',\n",
       "  'shop for food',\n",
       "  'agriculture and food systems',\n",
       "  'shopper intercept survey',\n",
       "  'food purchasing habits',\n",
       "  'teams including production and distribution',\n",
       "  'food budget to meat fish',\n",
       "  'us census bureau poverty threshold',\n",
       "  'household and food purchasing',\n",
       "  'rural urban food patterns',\n",
       "  'federal nutrition program participation for the intercept survey',\n",
       "  'national population sample',\n",
       "  'rural urban continuum codes',\n",
       "  'shopping and purchasing trends',\n",
       "  'food retail sector',\n",
       "  'consumer expenditure survey'],\n",
       " '2100032a-7c33-4bff-97ef-690822c43466': ['genomes project reference panels',\n",
       "  'alzheimer s disease neuroimaging initiative adni',\n",
       "  'wave of data collection in',\n",
       "  'cogent1 studies snps',\n",
       "  'initial discovery cohort',\n",
       "  'in genomic epidemiology',\n",
       "  'lothian birth cohort study',\n",
       "  'our framingham heart study']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Roberta - epoch 0\n",
    "res = remove_few_word_prediction(prediction_dict=results_1)\n",
    "for i in res.keys():\n",
    "    print(len(res[i]))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>sea level rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>coastal erosion beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>slr to natural resources habitats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>floodplain mapping program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>coastal erosion study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>cogent1 studies snps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>initial discovery cohort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>in genomic epidemiology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>lothian birth cohort study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>our framingham heart study</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Id                   PredictionString\n",
       "0   3f316b38-1a24-45a9-8d8c-4e05a42257c6                     sea level rise\n",
       "0   3f316b38-1a24-45a9-8d8c-4e05a42257c6              coastal erosion beach\n",
       "0   3f316b38-1a24-45a9-8d8c-4e05a42257c6  slr to natural resources habitats\n",
       "0   3f316b38-1a24-45a9-8d8c-4e05a42257c6         floodplain mapping program\n",
       "0   3f316b38-1a24-45a9-8d8c-4e05a42257c6              coastal erosion study\n",
       "..                                   ...                                ...\n",
       "3   2100032a-7c33-4bff-97ef-690822c43466               cogent1 studies snps\n",
       "3   2100032a-7c33-4bff-97ef-690822c43466           initial discovery cohort\n",
       "3   2100032a-7c33-4bff-97ef-690822c43466            in genomic epidemiology\n",
       "3   2100032a-7c33-4bff-97ef-690822c43466         lothian birth cohort study\n",
       "3   2100032a-7c33-4bff-97ef-690822c43466         our framingham heart study\n",
       "\n",
       "[261 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.DataFrame({'Id':list(res.keys()),\n",
    "                              'PredictionString':list(res.values())})\n",
    "prediction_df = prediction_df.explode('PredictionString')\n",
    "prediction_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
