{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'Azure'\n",
    "model_name = 'model_azure_train_sage_labels_bert_base_uncased.bin'\n",
    "\n",
    "if platform == 'Azure':\n",
    "    bert_path = '/home/thanish/transformer_models/bert_base_uncased'\n",
    "    test_path = '../test/*'\n",
    "    model_path = '../output/'\n",
    "elif platform == 'Kaggle':\n",
    "    bert_path = '../input/bertlargeuncasedpytorch'\n",
    "    test_path = '/kaggle/input/coleridgeinitiative-show-us-the-data/test/*'\n",
    "    model_path = '../input/coleridgemodels/'\n",
    "else:\n",
    "    bert_path = 'C:/Users/thanisb/Documents/transformer_models/bert_base_uncased/'\n",
    "    test_path = '../test/*'\n",
    "    model_path = '../output/'\n",
    "    \n",
    "config = {'MAX_LEN':128,\n",
    "          'tokenizer': AutoTokenizer.from_pretrained('bert-base-uncased' , do_lower_case=True),\n",
    "          'batch_size':64,\n",
    "          'Epoch': 15,\n",
    "          'test_path':test_path, \n",
    "          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "          'model_path':model_path,\n",
    "          'model_name':model_name\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_joining(data_dict_id):\n",
    "    '''\n",
    "    This function is to join all the text data from different sections in the json to a single\n",
    "    text file. \n",
    "    '''\n",
    "    data_length = len(data_dict_id)\n",
    "\n",
    "    #     temp = [clean_text(data_dict_id[i]['text']) for i in range(data_length)]\n",
    "    temp = [data_dict_id[i]['text'] for i in range(data_length)]\n",
    "    temp = '. '.join(temp)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>train_sentences</th>\n",
       "      <th>kword</th>\n",
       "      <th>label</th>\n",
       "      <th>sent_len</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007f880-0a9b-492d-9a58-76eb0b0e0bd7</td>\n",
       "      <td>in fact organizations are now identifying digi...</td>\n",
       "      <td>['program for the international assessment of ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>45</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008656f-0ba2-4632-8602-3017b44c2e90</td>\n",
       "      <td>besides not enough young people are entering s...</td>\n",
       "      <td>['trends in international mathematics and scie...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>94</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>1 manages access to results of the agricultura...</td>\n",
       "      <td>['agricultural resources management survey']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...</td>\n",
       "      <td>26</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>the agricultural resources management survey a...</td>\n",
       "      <td>['agricultural resources management survey']</td>\n",
       "      <td>['O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>29</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000efc17-13d8-433d-8f62-a3932fe4f3b8</td>\n",
       "      <td>genetic and neuroimaging data on a sub sample ...</td>\n",
       "      <td>['adni', 'alzheimer s disease neuroimaging ini...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>26</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89303</th>\n",
       "      <td>3158</td>\n",
       "      <td>the objectives of hbsc</td>\n",
       "      <td>['hbsc']</td>\n",
       "      <td>['O', 'O', 'O', 'B', 'B', 'B']</td>\n",
       "      <td>4</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89304</th>\n",
       "      <td>3158</td>\n",
       "      <td>hbsc contains extensive information on substan...</td>\n",
       "      <td>['hbsc']</td>\n",
       "      <td>['B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>9</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89305</th>\n",
       "      <td>3158</td>\n",
       "      <td>detailed description of hbsc</td>\n",
       "      <td>['hbsc']</td>\n",
       "      <td>['O', 'O', 'O', 'B', 'B', 'B']</td>\n",
       "      <td>4</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89306</th>\n",
       "      <td>3158</td>\n",
       "      <td>because hbsc sur</td>\n",
       "      <td>['hbsc']</td>\n",
       "      <td>['O', 'B', 'B', 'B', 'O']</td>\n",
       "      <td>3</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89307</th>\n",
       "      <td>3158</td>\n",
       "      <td>iannotti rj 2013 health behavior in school age...</td>\n",
       "      <td>['hbsc', 'health behavior in school aged child...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', ...</td>\n",
       "      <td>12</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89308 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0      0007f880-0a9b-492d-9a58-76eb0b0e0bd7   \n",
       "1      0008656f-0ba2-4632-8602-3017b44c2e90   \n",
       "2      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "3      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "4      000efc17-13d8-433d-8f62-a3932fe4f3b8   \n",
       "...                                     ...   \n",
       "89303                                  3158   \n",
       "89304                                  3158   \n",
       "89305                                  3158   \n",
       "89306                                  3158   \n",
       "89307                                  3158   \n",
       "\n",
       "                                         train_sentences  \\\n",
       "0      in fact organizations are now identifying digi...   \n",
       "1      besides not enough young people are entering s...   \n",
       "2      1 manages access to results of the agricultura...   \n",
       "3      the agricultural resources management survey a...   \n",
       "4      genetic and neuroimaging data on a sub sample ...   \n",
       "...                                                  ...   \n",
       "89303                             the objectives of hbsc   \n",
       "89304  hbsc contains extensive information on substan...   \n",
       "89305                       detailed description of hbsc   \n",
       "89306                                   because hbsc sur   \n",
       "89307  iannotti rj 2013 health behavior in school age...   \n",
       "\n",
       "                                                   kword  \\\n",
       "0      ['program for the international assessment of ...   \n",
       "1      ['trends in international mathematics and scie...   \n",
       "2           ['agricultural resources management survey']   \n",
       "3           ['agricultural resources management survey']   \n",
       "4      ['adni', 'alzheimer s disease neuroimaging ini...   \n",
       "...                                                  ...   \n",
       "89303                                           ['hbsc']   \n",
       "89304                                           ['hbsc']   \n",
       "89305                                           ['hbsc']   \n",
       "89306                                           ['hbsc']   \n",
       "89307  ['hbsc', 'health behavior in school aged child...   \n",
       "\n",
       "                                                   label  sent_len data_type  \n",
       "0      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        45     train  \n",
       "1      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        94     train  \n",
       "2      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...        26     train  \n",
       "3      ['O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', ...        29     train  \n",
       "4      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        26     train  \n",
       "...                                                  ...       ...       ...  \n",
       "89303                     ['O', 'O', 'O', 'B', 'B', 'B']         4      sage  \n",
       "89304  ['B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', ...         9      sage  \n",
       "89305                     ['O', 'O', 'O', 'B', 'B', 'B']         4      sage  \n",
       "89306                          ['O', 'B', 'B', 'B', 'O']         3      sage  \n",
       "89307  ['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', ...        12      sage  \n",
       "\n",
       "[89308 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_df = pd.read_csv(\"../labelled_data/unique_train_df_5.csv\")\n",
    "unique_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping all the sentences less than 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>train_sentences</th>\n",
       "      <th>kword</th>\n",
       "      <th>label</th>\n",
       "      <th>sent_len</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007f880-0a9b-492d-9a58-76eb0b0e0bd7</td>\n",
       "      <td>in fact organizations are now identifying digi...</td>\n",
       "      <td>['program for the international assessment of ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>45</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008656f-0ba2-4632-8602-3017b44c2e90</td>\n",
       "      <td>besides not enough young people are entering s...</td>\n",
       "      <td>['trends in international mathematics and scie...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>94</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>1 manages access to results of the agricultura...</td>\n",
       "      <td>['agricultural resources management survey']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...</td>\n",
       "      <td>26</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>the agricultural resources management survey a...</td>\n",
       "      <td>['agricultural resources management survey']</td>\n",
       "      <td>['O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>29</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000efc17-13d8-433d-8f62-a3932fe4f3b8</td>\n",
       "      <td>genetic and neuroimaging data on a sub sample ...</td>\n",
       "      <td>['adni', 'alzheimer s disease neuroimaging ini...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>26</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89303</th>\n",
       "      <td>3158</td>\n",
       "      <td>the objectives of hbsc</td>\n",
       "      <td>['hbsc']</td>\n",
       "      <td>['O', 'O', 'O', 'B', 'B', 'B']</td>\n",
       "      <td>4</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89304</th>\n",
       "      <td>3158</td>\n",
       "      <td>hbsc contains extensive information on substan...</td>\n",
       "      <td>['hbsc']</td>\n",
       "      <td>['B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>9</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89305</th>\n",
       "      <td>3158</td>\n",
       "      <td>detailed description of hbsc</td>\n",
       "      <td>['hbsc']</td>\n",
       "      <td>['O', 'O', 'O', 'B', 'B', 'B']</td>\n",
       "      <td>4</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89306</th>\n",
       "      <td>3158</td>\n",
       "      <td>because hbsc sur</td>\n",
       "      <td>['hbsc']</td>\n",
       "      <td>['O', 'B', 'B', 'B', 'O']</td>\n",
       "      <td>3</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89307</th>\n",
       "      <td>3158</td>\n",
       "      <td>iannotti rj 2013 health behavior in school age...</td>\n",
       "      <td>['hbsc', 'health behavior in school aged child...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', ...</td>\n",
       "      <td>12</td>\n",
       "      <td>sage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89308 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0      0007f880-0a9b-492d-9a58-76eb0b0e0bd7   \n",
       "1      0008656f-0ba2-4632-8602-3017b44c2e90   \n",
       "2      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "3      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "4      000efc17-13d8-433d-8f62-a3932fe4f3b8   \n",
       "...                                     ...   \n",
       "89303                                  3158   \n",
       "89304                                  3158   \n",
       "89305                                  3158   \n",
       "89306                                  3158   \n",
       "89307                                  3158   \n",
       "\n",
       "                                         train_sentences  \\\n",
       "0      in fact organizations are now identifying digi...   \n",
       "1      besides not enough young people are entering s...   \n",
       "2      1 manages access to results of the agricultura...   \n",
       "3      the agricultural resources management survey a...   \n",
       "4      genetic and neuroimaging data on a sub sample ...   \n",
       "...                                                  ...   \n",
       "89303                             the objectives of hbsc   \n",
       "89304  hbsc contains extensive information on substan...   \n",
       "89305                       detailed description of hbsc   \n",
       "89306                                   because hbsc sur   \n",
       "89307  iannotti rj 2013 health behavior in school age...   \n",
       "\n",
       "                                                   kword  \\\n",
       "0      ['program for the international assessment of ...   \n",
       "1      ['trends in international mathematics and scie...   \n",
       "2           ['agricultural resources management survey']   \n",
       "3           ['agricultural resources management survey']   \n",
       "4      ['adni', 'alzheimer s disease neuroimaging ini...   \n",
       "...                                                  ...   \n",
       "89303                                           ['hbsc']   \n",
       "89304                                           ['hbsc']   \n",
       "89305                                           ['hbsc']   \n",
       "89306                                           ['hbsc']   \n",
       "89307  ['hbsc', 'health behavior in school aged child...   \n",
       "\n",
       "                                                   label  sent_len data_type  \n",
       "0      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        45     train  \n",
       "1      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        94     train  \n",
       "2      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...        26     train  \n",
       "3      ['O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', ...        29     train  \n",
       "4      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        26     train  \n",
       "...                                                  ...       ...       ...  \n",
       "89303                     ['O', 'O', 'O', 'B', 'B', 'B']         4      sage  \n",
       "89304  ['B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', ...         9      sage  \n",
       "89305                     ['O', 'O', 'O', 'B', 'B', 'B']         4      sage  \n",
       "89306                          ['O', 'B', 'B', 'B', 'O']         3      sage  \n",
       "89307  ['O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', ...        12      sage  \n",
       "\n",
       "[89308 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_df = unique_df.loc[unique_df.sent_len<512, :].reset_index(drop=True)\n",
    "unique_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take unqiue of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86828, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_df = unique_df.drop_duplicates()\n",
    "unique_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking the sample of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86828, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique_df = unique_df.sample(int(unique_df.shape[0]*0.5)).reset_index(drop=True)\n",
    "unique_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36058, 6) (50770, 6)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "train_df, valid_df = train_test_split(unique_df, test_size=0.05)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape, valid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the DataFrame back to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_2_idx = {'O': 0 , 'B': 1, 'P': 2}\n",
    "\n",
    "def dataset_2_list(df):\n",
    "    id_list = df.id.values.tolist()\n",
    "    sentences_list = df.train_sentences.values.tolist()\n",
    "    keywords_list = df.kword.apply(lambda x : eval(x)).values.tolist()\n",
    "    \n",
    "    labels_list = df.label.apply(lambda x : eval(x)).values.tolist()    \n",
    "    labels_list = [list(map(tags_2_idx.get, lab)) for lab in labels_list]\n",
    "    \n",
    "    return id_list, sentences_list, keywords_list, labels_list\n",
    "\n",
    "final_train_id_list, final_train_sentences, final_train_keywords, final_train_labels = dataset_2_list(df=train_df)\n",
    "final_valid_id_list, final_valid_sentences, final_valid_keywords, final_valid_labels = dataset_2_list(df=valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # verification\n",
    "# ind = 800\n",
    "# final_train_sentences[ind], config['tokenizer'].tokenize(final_train_sentences[ind]), final_train_keywords[ind], final_train_labels[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forming the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class form_input():\n",
    "    def __init__(self, ID, sentence, kword, label, data_type='test'):\n",
    "        self.id = ID\n",
    "        self.sentence = sentence\n",
    "        self.kword = kword\n",
    "        self.label = label\n",
    "        self.max_length = config['MAX_LEN']\n",
    "        self.tokenizer = config['tokenizer']\n",
    "        self.data_type = data_type\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        toks = config['tokenizer'].tokenize(self.sentence[item])\n",
    "        \n",
    "        if len(toks)>self.max_length:\n",
    "            toks = toks[:self.max_length]\n",
    "            \n",
    "        ########################################\n",
    "        # Forming the inputs\n",
    "        ids = config['tokenizer'].convert_tokens_to_ids(toks)\n",
    "        tok_type_id = [0] * len(ids)\n",
    "        att_mask = [1] * len(ids)\n",
    "        \n",
    "        # Padding\n",
    "        pad_len = self.max_length - len(ids)        \n",
    "        ids = ids + [2] * pad_len\n",
    "        tok_type_id = tok_type_id + [0] * pad_len\n",
    "        att_mask = att_mask + [0] * pad_len\n",
    "        \n",
    "        ########################################\n",
    "        # Forming the label\n",
    "        if self.data_type != 'test':\n",
    "            label = self.label[item] \n",
    "            if len(label)>self.max_length:\n",
    "                label = label[:self.max_length]\n",
    "            else:\n",
    "                label = label + [2] * pad_len\n",
    "\n",
    "        else:\n",
    "            label = 1\n",
    "        ########################################\n",
    "        \n",
    "#         print(item, len(ids), len(tok_type_id), len(att_mask), len(label))\n",
    "        \n",
    "        return {'pub_id': self.id[item],\n",
    "                'item': item,\n",
    "                #'sentence': self.sentence[item],\n",
    "                #'kword' : self.kword[item],\n",
    "                'ids': torch.tensor(ids, dtype = torch.long),\n",
    "                'tok_type_id': torch.tensor(tok_type_id, dtype = torch.long),\n",
    "                'att_mask': torch.tensor(att_mask, dtype = torch.long),\n",
    "                'target': torch.tensor(label, dtype = torch.long)\n",
    "               }\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod_input = form_input(ID=final_train_id_list, \n",
    "                              sentence=final_train_sentences, \n",
    "                              kword=final_train_keywords, \n",
    "                              label=final_train_labels, \n",
    "                              data_type='train')\n",
    "\n",
    "valid_prod_input = form_input(ID=final_valid_id_list, \n",
    "                              sentence=final_valid_sentences, \n",
    "                              kword=final_valid_keywords, \n",
    "                              label=final_valid_labels, \n",
    "                              data_type='valid')\n",
    "\n",
    "train_prod_input_data_loader = DataLoader(train_prod_input, \n",
    "                                          batch_size= config['batch_size'], \n",
    "                                          shuffle=True)\n",
    "\n",
    "valid_prod_input_data_loader = DataLoader(valid_prod_input, \n",
    "                                          batch_size= config['batch_size'], \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ind = 8443\n",
    "# train_prod_input[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 128, 128)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 360\n",
    "len(train_prod_input[ind]['ids']), len(train_prod_input[ind]['att_mask']), len(train_prod_input[ind]['tok_type_id']), len(train_prod_input[ind]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    actual_flat = labels.flatten()\n",
    "    pred_flat = preds.flatten()\n",
    "    \n",
    "    # Get all non-padded index\n",
    "    non_padded_index = np.where(actual_flat != tags_2_idx[\"P\"])[0]\n",
    "    \n",
    "    # Only non-padded positions for the actual and the predicted\n",
    "    actual_flat = actual_flat[non_padded_index]\n",
    "    pred_flat = pred_flat[non_padded_index]\n",
    "    \n",
    "    print(confusion_matrix(actual_flat, pred_flat))\n",
    "    \n",
    "    # Accuracy\n",
    "    Accuracy = np.sum(actual_flat == pred_flat ) / len(pred_flat )\n",
    "    \n",
    "    # Recall calculator\n",
    "    condition = (actual_flat == 1)\n",
    "    actual_index = np.where(condition)[0]\n",
    "    actual_flat_rec = actual_flat[actual_index]\n",
    "    pred_flat_rec = pred_flat[actual_index]\n",
    "    Recall = np.sum(actual_flat_rec == pred_flat_rec) / len(pred_flat_rec)\n",
    "    \n",
    "    # Precision calculator\n",
    "    condition = (pred_flat == 1)\n",
    "    pred_index = np.where(condition)[0]\n",
    "    actual_flat_prc = actual_flat[pred_index]\n",
    "    pred_flat_prc = pred_flat[pred_index]\n",
    "    Precision = np.sum(actual_flat_prc == pred_flat_prc) / len(pred_flat_prc)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    common_index = np.intersect1d(actual_index, pred_index)\n",
    "    jaccard_similarity = float(len(common_index))/( len(actual_index) + len(pred_index) - len(common_index))\n",
    "    \n",
    "    return Accuracy, Recall, Precision, jaccard_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for index, dataset in enumerate(tqdm(data_loader, total = len(data_loader))):\n",
    "        batch_input_ids = dataset['ids'].to(config['device'], dtype = torch.long)\n",
    "        batch_att_mask = dataset['att_mask'].to(config['device'], dtype = torch.long)\n",
    "        batch_tok_type_id = dataset['tok_type_id'].to(config['device'], dtype = torch.long)\n",
    "        batch_target = dataset['target'].to(config['device'], dtype = torch.long)\n",
    "                \n",
    "        model.zero_grad()\n",
    "        output = model(batch_input_ids, \n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=batch_att_mask,\n",
    "                       labels=batch_target)\n",
    "        \n",
    "        step_loss = output[0]\n",
    "        prediction = output[1]\n",
    "        \n",
    "        step_loss.sum().backward()\n",
    "        optimizer.step()        \n",
    "        train_loss += step_loss\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    tr_loss = train_loss.sum()/len(data_loader)\n",
    "    \n",
    "    return tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    predictions = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])\n",
    "    true_labels = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, dataset in enumerate(tqdm(data_loader, total = len(data_loader))):\n",
    "            batch_input_ids = dataset['ids'].to(config['device'], dtype = torch.long)\n",
    "            batch_att_mask = dataset['att_mask'].to(config['device'], dtype = torch.long)\n",
    "            batch_tok_type_id = dataset['tok_type_id'].to(config['device'], dtype = torch.long)\n",
    "            batch_target = dataset['target'].to(config['device'], dtype = torch.long)\n",
    "\n",
    "            output = model(batch_input_ids, \n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=batch_att_mask,\n",
    "                           labels=batch_target)\n",
    "\n",
    "            step_loss = output[0]\n",
    "            eval_prediction = output[1]\n",
    "\n",
    "            eval_loss += step_loss\n",
    "            \n",
    "            eval_prediction = np.argmax(eval_prediction.detach().to('cpu').numpy(), axis = 2)\n",
    "            actual = batch_target.to('cpu').numpy()\n",
    "            \n",
    "            predictions = np.concatenate((predictions, eval_prediction), axis = 0)\n",
    "            true_labels = np.concatenate((true_labels, actual), axis = 0)\n",
    "           \n",
    "        ev_loss = eval_loss.sum()/len(data_loader)\n",
    "        Accuracy, Recall, Precision, jaccard_similarity = flat_accuracy(preds=predictions , labels=true_labels)\n",
    "        \n",
    "        print(f'Eval Accuracy: {Accuracy}, Recall: {Recall}, Precision: {Precision}, jaccard_similarity: {jaccard_similarity}')\n",
    "            \n",
    "    return ev_loss, Precision, predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_engine(epoch, train_data, valid_data):\n",
    "    model = transformers.BertForTokenClassification.from_pretrained('bert-base-uncased',  num_labels = len(tags_2_idx))\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(config['device'])\n",
    "    \n",
    "    params = model.parameters()\n",
    "    optimizer = torch.optim.Adam(params, lr= 3e-5)\n",
    "    \n",
    "    best_eval_loss = 1000000\n",
    "    best_Precision = 0\n",
    "    for i in range(epoch):\n",
    "        train_loss = train_fn(data_loader = train_data, \n",
    "                              model=model, \n",
    "                              optimizer=optimizer)\n",
    "        eval_loss, Precision, eval_predictions, true_labels = eval_fn(data_loader = valid_data, \n",
    "                                                                      model=model)\n",
    "        \n",
    "        print(f\"Epoch {i} , Train loss: {train_loss}, Eval loss: {eval_loss}\")\n",
    "\n",
    "#         if Precision > best_Precision:\n",
    "#             best_Precision = Precision           \n",
    "        \n",
    "        saving_name = config['model_path'] + f\"epoch_{i}_\" + config['model_name']\n",
    "        print(\"Saving the model:\", saving_name)\n",
    "        torch.save(model.state_dict(), saving_name)\n",
    "            \n",
    "    return model, eval_predictions, true_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [13:03<00:00,  1.39s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 794/794 [07:35<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1747785   89508]\n",
      " [  90067  119773]]\n",
      "Eval Accuracy: 0.912279759058156, Recall: 0.5707825009531071, Precision: 0.5723070895112313, jaccard_similarity: 0.4001129120622152\n",
      "Epoch 0 , Train loss: 0.24241702258586884, Eval loss: 0.4875009059906006\n",
      "Saving the model: ../output/epoch_0_model_azure_train_sage_labels_bert_base_uncased.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [13:08<00:00,  1.40s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 794/794 [07:33<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1745039   92254]\n",
      " [  94045  115795]]\n",
      "Eval Accuracy: 0.9089951654338042, Recall: 0.5518252001524971, Precision: 0.5565756143985311, jaccard_similarity: 0.3833078445781776\n",
      "Epoch 1 , Train loss: 0.11769293248653412, Eval loss: 0.5301032662391663\n",
      "Saving the model: ../output/epoch_1_model_azure_train_sage_labels_bert_base_uncased.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [13:08<00:00,  1.40s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 794/794 [07:33<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1764869   72424]\n",
      " [ 112806   97034]]\n",
      "Eval Accuracy: 0.9095173591554628, Recall: 0.4624189858940145, Precision: 0.5726138630221058, jaccard_similarity: 0.3437703710001984\n",
      "Epoch 2 , Train loss: 0.08714748173952103, Eval loss: 0.6763190627098083\n",
      "Saving the model: ../output/epoch_2_model_azure_train_sage_labels_bert_base_uncased.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [13:08<00:00,  1.40s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 794/794 [07:33<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1734176  103117]\n",
      " [  99077  110763]]\n",
      "Eval Accuracy: 0.9012306479354297, Recall: 0.5278450247807853, Precision: 0.5178745090705068, jaccard_similarity: 0.3539240215109424\n",
      "Epoch 3 , Train loss: 0.06999390572309494, Eval loss: 0.7602826356887817\n",
      "Saving the model: ../output/epoch_3_model_azure_train_sage_labels_bert_base_uncased.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [13:08<00:00,  1.40s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 794/794 [07:33<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1757345   79948]\n",
      " [ 113508   96332]]\n",
      "Eval Accuracy: 0.9054990564853382, Recall: 0.45907357987037745, Precision: 0.5464715225777173, jaccard_similarity: 0.3324223225254324\n",
      "Epoch 4 , Train loss: 0.05723828077316284, Eval loss: 0.8454709649085999\n",
      "Saving the model: ../output/epoch_4_model_azure_train_sage_labels_bert_base_uncased.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [13:08<00:00,  1.40s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 794/794 [08:16<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1780809   56484]\n",
      " [ 121902   87938]]\n",
      "Eval Accuracy: 0.9128605713453889, Recall: 0.41907167365611897, Precision: 0.6088961515558572, jaccard_similarity: 0.3301917964584491\n",
      "Epoch 5 , Train loss: 0.04868071526288986, Eval loss: 0.8645114302635193\n",
      "Saving the model: ../output/epoch_5_model_azure_train_sage_labels_bert_base_uncased.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [13:08<00:00,  1.40s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 794/794 [07:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1773784   63509]\n",
      " [ 128580   81260]]\n",
      "Eval Accuracy: 0.9061668196448399, Recall: 0.38724742661075107, Precision: 0.5613080148374306, jaccard_similarity: 0.2972756439569927\n",
      "Epoch 6 , Train loss: 0.0431593619287014, Eval loss: 0.9172398447990417\n",
      "Saving the model: ../output/epoch_6_model_azure_train_sage_labels_bert_base_uncased.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [13:08<00:00,  1.40s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 794/794 [07:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1767129   70164]\n",
      " [ 116017   93823]]\n",
      "Eval Accuracy: 0.9090528070232857, Recall: 0.44711685093404496, Precision: 0.5721368157231975, jaccard_similarity: 0.3350773560377709\n",
      "Epoch 7 , Train loss: 0.035998668521642685, Eval loss: 0.8792698383331299\n",
      "Saving the model: ../output/epoch_7_model_azure_train_sage_labels_bert_base_uncased.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 564/564 [13:08<00:00,  1.40s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 794/794 [07:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1771323   65970]\n",
      " [ 125515   84325]]\n",
      "Eval Accuracy: 0.9064618664248977, Recall: 0.4018537933663744, Precision: 0.5610632422901627, jaccard_similarity: 0.3057358326384105\n",
      "Epoch 8 , Train loss: 0.03218204900622368, Eval loss: 1.1767735481262207\n",
      "Saving the model: ../output/epoch_8_model_azure_train_sage_labels_bert_base_uncased.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 276/564 [06:25<06:43,  1.40s/it]"
     ]
    }
   ],
   "source": [
    "model, eval_predictions, eval_true_labels = train_engine(epoch=config['Epoch'], \n",
    "                                                         train_data=train_prod_input_data_loader, \n",
    "                                                         valid_data=valid_prod_input_data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
