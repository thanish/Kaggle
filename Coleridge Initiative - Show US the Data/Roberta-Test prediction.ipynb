{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'Azure'\n",
    "model_name = 'epoch_2_model_azure_roberta_base_cleaned_xtra_label.bin'\n",
    "\n",
    "if platform == 'Azure':\n",
    "    bert_path = '/home/thanish/transformer_models/bert_base_uncased'\n",
    "    test_path = '../test/*'\n",
    "    model_path = '../output/'\n",
    "elif platform == 'Kaggle':\n",
    "    bert_path = '../input/bertlargeuncasedpytorch'\n",
    "    test_path = '/kaggle/input/coleridgeinitiative-show-us-the-data/test/*'\n",
    "    model_path = '../input/coleridgemodels/'\n",
    "else:\n",
    "    bert_path = 'C:/Users/thanisb/Documents/transformer_models/bert_base_uncased/'\n",
    "    test_path = '../test/*'\n",
    "    model_path = '../output/'\n",
    "    \n",
    "config = {'MAX_LEN':512,\n",
    "          'tokenizer': AutoTokenizer.from_pretrained('roberta-base' , do_lower_case=True),\n",
    "          'batch_size':20,\n",
    "          'Epoch': 3,\n",
    "          'test_path':test_path, \n",
    "          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "          'model_path':model_path,\n",
    "          'model_name':model_name\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_joining(data_dict_id):\n",
    "    '''\n",
    "    This function is to join all the text data from different sections in the json to a single\n",
    "    text file. \n",
    "    '''\n",
    "    data_length = len(data_dict_id)\n",
    "\n",
    "    #     temp = [clean_text(data_dict_id[i]['text']) for i in range(data_length)]\n",
    "    temp = [data_dict_id[i]['text'] for i in range(data_length)]\n",
    "    temp = '. '.join(temp)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_json(test_data_folder):\n",
    "    '''\n",
    "    This function reads all the json input files and return a dictionary containing the id as the key\n",
    "    and all the contents of the json as values\n",
    "    '''\n",
    "\n",
    "    test_text_data = {}\n",
    "    total_files = len(glob.glob(test_data_folder))\n",
    "    \n",
    "    for i, test_json_loc in enumerate(glob.glob(test_data_folder)):\n",
    "        filename = test_json_loc.split(\"/\")[-1][:-5]\n",
    "\n",
    "        with open(test_json_loc, 'r') as f:\n",
    "            test_text_data[filename] = json.load(f)\n",
    "\n",
    "        if (i%1000) == 0:\n",
    "            print(f\"Completed {i}/{total_files}\")\n",
    "\n",
    "    print(\"All files read\")\n",
    "    return test_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0/4\n",
      "All files read\n"
     ]
    }
   ],
   "source": [
    "test_data_dict = read_test_json(test_data_folder=\"../test/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the saved model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model checkpoint: ../output/epoch_2_model_azure_roberta_base_cleaned_xtra_label.bin\n",
      "Checkpoint loaded\n",
      "Model loaded with all keys matching with the checkpoint\n"
     ]
    }
   ],
   "source": [
    "# initializing the model\n",
    "model = transformers.RobertaForTokenClassification.from_pretrained('roberta-base',  num_labels = 3)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "# Reading the trained checkpoint model\n",
    "trained_model_name = config['model_path'] + config['model_name']\n",
    "print(\"Trained model checkpoint:\", trained_model_name)\n",
    "checkpoint = torch.load(trained_model_name, map_location = config['device'])\n",
    "print(\"Checkpoint loaded\")\n",
    "\n",
    "# Matching the trained checkpoint model to the initialized model\n",
    "model.load_state_dict(checkpoint)\n",
    "print(\"Model loaded with all keys matching with the checkpoint\")\n",
    "\n",
    "model = model.to(config['device'])\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def prediction_fn(tokenized_sub_sentence):\n",
    "\n",
    "    tkns = tokenized_sub_sentence\n",
    "    indexed_tokens = config['tokenizer'].convert_tokens_to_ids(tkns)\n",
    "    segments_ids = [0] * len(indexed_tokens)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]).to(config['device'])\n",
    "    segments_tensors = torch.tensor([segments_ids]).to(config['device'])\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit = model(tokens_tensor, \n",
    "                      token_type_ids=None,\n",
    "                      attention_mask=segments_tensors)\n",
    "\n",
    "        logit_new = logit[0].argmax(2).detach().cpu().numpy().tolist()\n",
    "        prediction = logit_new[0]\n",
    "\n",
    "#         print(tkns)\n",
    "#         print(logit_new)\n",
    "#         print(prediction)\n",
    "        \n",
    "        kword = ''\n",
    "        kword_list = []\n",
    "\n",
    "        for k, j in enumerate(prediction):\n",
    "            if (len(prediction)>1):\n",
    "\n",
    "                if (j!=0) & (k==0):\n",
    "                    #if it's the first word in the first position\n",
    "                    #print('At begin first word')\n",
    "                    begin = tkns[k]\n",
    "                    kword = begin\n",
    "\n",
    "                elif (j!=0) & (k>=1) & (prediction[k-1]==0):\n",
    "                    #begin word is in the middle of the sentence\n",
    "                    begin = tkns[k]\n",
    "                    previous = tkns[k-1]\n",
    "\n",
    "                    if not begin.startswith('Ġ'):\n",
    "                        kword = previous + begin[:]\n",
    "                    else:\n",
    "                        kword = begin\n",
    "\n",
    "                    if k == (len(prediction) - 1):\n",
    "                        #print('begin and end word is the last word of the sentence')\n",
    "                        kword_list.append(kword.rstrip().lstrip().replace('Ġ', ''))\n",
    "\n",
    "                elif (j!=0) & (k>=1) & (prediction[k-1]!=0):\n",
    "                    # intermediate word of the same keyword\n",
    "                    inter = tkns[k]\n",
    "\n",
    "                    if not inter.startswith('Ġ'):\n",
    "                        kword = kword + \"\" + inter[:]\n",
    "                    else:\n",
    "                        kword = kword + \" \" + inter\n",
    "\n",
    "\n",
    "                    if k == (len(prediction) - 1):\n",
    "                        #print('begin and end')\n",
    "                        kword_list.append(kword.rstrip().lstrip().replace('Ġ', ''))\n",
    "\n",
    "                elif (j==0) & (k>=1) & (prediction[k-1] !=0):\n",
    "                    # End of a keywords but not end of sentence.\n",
    "                    kword_list.append(kword.rstrip().lstrip().replace('Ġ', ''))\n",
    "                    kword = ''\n",
    "                    inter = ''\n",
    "            else:\n",
    "                if (j!=0):\n",
    "                    begin = tkns[k]\n",
    "                    kword = begin\n",
    "                    kword_list.append(kword.rstrip().lstrip().replace('Ġ', ''))\n",
    "#         print(kword_list)\n",
    "#         print(\"\")\n",
    "    return kword_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_sent_split(text):\n",
    "    sent_split = text.split(\" \")\n",
    "\n",
    "    start = 0\n",
    "    end = len(sent_split)\n",
    "    max_length = 64\n",
    "\n",
    "    final_sent_split = []\n",
    "    for i in range(start, end, max_length):\n",
    "        temp = sent_split[i: (i + max_length)]\n",
    "        final_sent_split.append(\" \".join(i for i in temp))\n",
    "    return final_sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data_dict):\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for i, Id in enumerate(data_dict.keys()):\n",
    "        current_id_predictions = []\n",
    "        \n",
    "        print(Id)\n",
    "        sentences = data_joining(data_dict[Id])\n",
    "        sentence_tokens = sent_tokenize(sentences)\n",
    "        \n",
    "        for sub_sentence in sentence_tokens:\n",
    "            cleaned_sub_sentence = clean_text(sub_sentence)\n",
    "        \n",
    "            # Tokenize the sentence\n",
    "            tokenized_sub_sentence = config['tokenizer'].tokenize(\" \" + cleaned_sub_sentence)\n",
    "            \n",
    "            if len(tokenized_sub_sentence) == 0:\n",
    "                # If the tokenized sentence are empty\n",
    "                sub_sentence_prediction_kword_list = []\n",
    "                \n",
    "            elif len(tokenized_sub_sentence) <= 512:\n",
    "                # If the tokenized sentence are less than 512\n",
    "                sub_sentence_prediction_kword_list = prediction_fn(tokenized_sub_sentence)\n",
    "\n",
    "            else:\n",
    "                # If the tokenized sentence are >512 which is long sentences\n",
    "                long_sent_kword_list = []\n",
    "                \n",
    "                tokenized_sub_sentence_tok_split = long_sent_split(text = tokenized_sub_sentence)\n",
    "                for i, sent_tok in enumerate(tokenized_sub_sentence_tok_split):\n",
    "                    if len(sent) != 0:\n",
    "                        kword_list = prediction_fn(sent_tok)\n",
    "                        long_sent_kword_list.append(kword_list)\n",
    "                flat_long_sent_kword = [item for sublist in long_sent_kword_list for item in sublist]\n",
    "                sub_sentence_prediction_kword_list = flat_long_sent_kword\n",
    "                            \n",
    "            if len(sub_sentence_prediction_kword_list) !=0:\n",
    "                current_id_predictions = current_id_predictions + sub_sentence_prediction_kword_list\n",
    "\n",
    "        results[Id] = list(set(current_id_predictions))\n",
    "                \n",
    "    print(\"All predictions completed\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_few_word_prediction(prediction_dict):\n",
    "    final_result = {}\n",
    "    for ID in prediction_dict.keys():\n",
    "        temp = []\n",
    "\n",
    "        for pred in prediction_dict[ID]:\n",
    "            pred_split = pred.split(\" \")\n",
    "            condition1 = len(pred_split)<=2\n",
    "            condition2 = 'adni' not in pred\n",
    "            condition3 = 'cccsl' not in pred\n",
    "            condition4 = 'ibtracs' not in pred\n",
    "            condition5 = 'slosh model' not in pred\n",
    "            \n",
    "            if condition1 & condition2 & condition3 & condition4 & condition5:\n",
    "                pass\n",
    "            else:\n",
    "                temp.append(pred)\n",
    "        final_result[ID] = temp\n",
    "        \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_1 = get_predictions(data_dict = test_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "38\n",
      "34\n",
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2100032a-7c33-4bff-97ef-690822c43466': ['heart and aging research in genomic epidemiology',\n",
       "  'helsinki birth cohort',\n",
       "  'ssgac cognition study',\n",
       "  'adni',\n",
       "  'affymetrix genome wide snp'],\n",
       " '2f392438-e215-4169-bebf-21ac4ff253e1': ['education secondary education and higher',\n",
       "  'center for education statistics institute',\n",
       "  'university diploma of specialized higher studies',\n",
       "  'ppps exchange rate data',\n",
       "  'international standard classification of education',\n",
       "  'current population survey',\n",
       "  'national education systems ines survey',\n",
       "  'end of upper secondary education',\n",
       "  'certificates of higher education',\n",
       "  'progress in international reading literacy study',\n",
       "  'international data base',\n",
       "  'pisa data collection effort',\n",
       "  'education combined field of study',\n",
       "  'program for international student assessment',\n",
       "  'online education database',\n",
       "  'integrated postsecondary education data system',\n",
       "  'oecd national accounts database',\n",
       "  'high school for upper secondary school',\n",
       "  'development in science',\n",
       "  'schools and staffing survey',\n",
       "  'education system in',\n",
       "  'common of secondary education',\n",
       "  'professional development in science assessment',\n",
       "  'international standard classification of education major field of study',\n",
       "  'the science mathematics and engineering',\n",
       "  'education system in japan',\n",
       "  'professional development in science',\n",
       "  'trends in international mathematics and science study',\n",
       "  'international socioeconomic index of occupational status',\n",
       "  'secondary schools further education colleges and training centers',\n",
       "  'further education sector',\n",
       "  'international standard classification of education is',\n",
       "  'professional development in science pedagogy',\n",
       "  'general certificate of secondary education qualifications',\n",
       "  'medical doctor dental pharmacy',\n",
       "  'development in mathematics',\n",
       "  'education system of germany',\n",
       "  'digest of education statistics'],\n",
       " '3f316b38-1a24-45a9-8d8c-4e05a42257c6': ['nc sea level rise risk management study',\n",
       "  'coastal resources commission',\n",
       "  'noaa storm surge inundation mapping guide nc',\n",
       "  'coastal flood risk',\n",
       "  'shoreline change maps',\n",
       "  'utilizing geographic information systems',\n",
       "  'slosh model',\n",
       "  'coastal observation station',\n",
       "  'county and nps building data',\n",
       "  'north carolina floodplain mapping program',\n",
       "  'shoreline change assessment',\n",
       "  'storm surge community vulnerability assessment',\n",
       "  'nc floodplain mapping program',\n",
       "  'caha historic resource',\n",
       "  'usgs coastal vulnerability index',\n",
       "  'hurricane preparedness and safety open house',\n",
       "  'coastal flood risk study',\n",
       "  'lidar building and infrastructure data',\n",
       "  'sea lake and overland surges from hurricanes',\n",
       "  'university coastal research center',\n",
       "  'utm zone 18 north meters coordinate system',\n",
       "  'national hydrography dataset',\n",
       "  'nc flood risk information system',\n",
       "  'coastal erosion study',\n",
       "  'coastal change hazards portal',\n",
       "  'natural neighbor interpolation',\n",
       "  'slosh display program',\n",
       "  'slosh inundation grid',\n",
       "  'coastal change science',\n",
       "  'north carolina emergency management spatial data download portal',\n",
       "  'usgs digital shoreline analysis system',\n",
       "  'nc coastal erosion study',\n",
       "  'sea level affecting marshes',\n",
       "  'las dataset data management in arcgis 10'],\n",
       " '8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60': ['homescan and ces',\n",
       "  'national food survey',\n",
       "  'our intercept survey',\n",
       "  'shopper intercept survey',\n",
       "  'what we eat in america',\n",
       "  'iri data analysis',\n",
       "  'customer intercept survey',\n",
       "  'iri consumer network panel data',\n",
       "  'national dataset foodaps',\n",
       "  'urban food patterns',\n",
       "  'civilian non institutionalized population bureau of labor',\n",
       "  'rural urban continuum codes',\n",
       "  'current retail trends',\n",
       "  'agriculture and food',\n",
       "  'food access research atlas',\n",
       "  'cnp and intercept survey']}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Roberta - epoch 0\n",
    "res = remove_few_word_prediction(prediction_dict=results_1)\n",
    "for i in res.keys():\n",
    "    print(len(res[i]))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>heart and aging research in genomic epidemiology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>helsinki birth cohort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>ssgac cognition study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>adni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>affymetrix genome wide snp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>rural urban continuum codes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>current retail trends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>agriculture and food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>food access research atlas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>cnp and intercept survey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Id  \\\n",
       "0   2100032a-7c33-4bff-97ef-690822c43466   \n",
       "0   2100032a-7c33-4bff-97ef-690822c43466   \n",
       "0   2100032a-7c33-4bff-97ef-690822c43466   \n",
       "0   2100032a-7c33-4bff-97ef-690822c43466   \n",
       "0   2100032a-7c33-4bff-97ef-690822c43466   \n",
       "..                                   ...   \n",
       "3   8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n",
       "3   8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n",
       "3   8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n",
       "3   8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n",
       "3   8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n",
       "\n",
       "                                    PredictionString  \n",
       "0   heart and aging research in genomic epidemiology  \n",
       "0                              helsinki birth cohort  \n",
       "0                              ssgac cognition study  \n",
       "0                                               adni  \n",
       "0                         affymetrix genome wide snp  \n",
       "..                                               ...  \n",
       "3                        rural urban continuum codes  \n",
       "3                              current retail trends  \n",
       "3                               agriculture and food  \n",
       "3                         food access research atlas  \n",
       "3                           cnp and intercept survey  \n",
       "\n",
       "[93 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.DataFrame({'Id':list(res.keys()),\n",
    "                              'PredictionString':list(res.values())})\n",
    "prediction_df = prediction_df.explode('PredictionString')\n",
    "prediction_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
