{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'Azure'\n",
    "model_name = 'albert_base_uncased_cleaned_extra_label_100per_data.bin'\n",
    "\n",
    "if platform == 'Azure':\n",
    "    bert_path = '/home/thanish/transformer_models/bert_base_uncased'\n",
    "    test_path = '../test/*'\n",
    "    model_path = '../output/'\n",
    "elif platform == 'Kaggle':\n",
    "    bert_path = '../input/bertlargeuncasedpytorch'\n",
    "    test_path = '/kaggle/input/coleridgeinitiative-show-us-the-data/test/*'\n",
    "    model_path = '../input/coleridgemodels/'\n",
    "else:\n",
    "    bert_path = 'C:/Users/thanisb/Documents/transformer_models/bert_base_uncased/'\n",
    "    test_path = '../test/*'\n",
    "    model_path = '../output/'\n",
    "    \n",
    "config = {'MAX_LEN':128,\n",
    "          'tokenizer': AutoTokenizer.from_pretrained('albert-base-v2' , do_lower_case=True),\n",
    "          'batch_size':64,\n",
    "          'Epoch': 4,\n",
    "          'test_path':test_path, \n",
    "          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "          'model_path':model_path,\n",
    "          'model_name':model_name\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_joining(data_dict_id):\n",
    "    '''\n",
    "    This function is to join all the text data from different sections in the json to a single\n",
    "    text file. \n",
    "    '''\n",
    "    data_length = len(data_dict_id)\n",
    "\n",
    "    #     temp = [clean_text(data_dict_id[i]['text']) for i in range(data_length)]\n",
    "    temp = [data_dict_id[i]['text'] for i in range(data_length)]\n",
    "    temp = '. '.join(temp)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>train_sentences</th>\n",
       "      <th>kword</th>\n",
       "      <th>label</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007f880-0a9b-492d-9a58-76eb0b0e0bd7</td>\n",
       "      <td>in fact organizations are now identifying digi...</td>\n",
       "      <td>['program for the international assessment of ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008656f-0ba2-4632-8602-3017b44c2e90</td>\n",
       "      <td>besides not enough young people are entering s...</td>\n",
       "      <td>['trends in international mathematics and scie...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>1 manages access to results of the agricultura...</td>\n",
       "      <td>['agricultural resources management survey']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>the agricultural resources management survey a...</td>\n",
       "      <td>['agricultural resources management survey']</td>\n",
       "      <td>['O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>the resulting statistics provide the fulcrum f...</td>\n",
       "      <td>['farm income and wealth statistics']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58673</th>\n",
       "      <td>ffd4d86a-0f26-44cc-baed-f0e209cc22af</td>\n",
       "      <td>data used in the preparation of this article w...</td>\n",
       "      <td>['alzheimer s disease neuroimaging initiative ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58674</th>\n",
       "      <td>ffe7f334-245a-4de7-b600-d7ff4e28bfca</td>\n",
       "      <td>interestingly the genome sequences of sars cov...</td>\n",
       "      <td>['genome sequences of sars cov 2']</td>\n",
       "      <td>['O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'B', ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58675</th>\n",
       "      <td>ffeb3568-7aed-4dbe-b177-cbd7f46f34af</td>\n",
       "      <td>as part of the program for international stude...</td>\n",
       "      <td>['trends in international mathematics and scie...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58676</th>\n",
       "      <td>ffee2676-a778-4521-b947-e1e420b126c5</td>\n",
       "      <td>analysis considered first time beginning posts...</td>\n",
       "      <td>['beginning postsecondary students']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'O', ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58677</th>\n",
       "      <td>ffee2676-a778-4521-b947-e1e420b126c5</td>\n",
       "      <td>my prior research illustrated with use of begi...</td>\n",
       "      <td>['beginning postsecondary student']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58678 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0      0007f880-0a9b-492d-9a58-76eb0b0e0bd7   \n",
       "1      0008656f-0ba2-4632-8602-3017b44c2e90   \n",
       "2      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "3      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "4      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "...                                     ...   \n",
       "58673  ffd4d86a-0f26-44cc-baed-f0e209cc22af   \n",
       "58674  ffe7f334-245a-4de7-b600-d7ff4e28bfca   \n",
       "58675  ffeb3568-7aed-4dbe-b177-cbd7f46f34af   \n",
       "58676  ffee2676-a778-4521-b947-e1e420b126c5   \n",
       "58677  ffee2676-a778-4521-b947-e1e420b126c5   \n",
       "\n",
       "                                         train_sentences  \\\n",
       "0      in fact organizations are now identifying digi...   \n",
       "1      besides not enough young people are entering s...   \n",
       "2      1 manages access to results of the agricultura...   \n",
       "3      the agricultural resources management survey a...   \n",
       "4      the resulting statistics provide the fulcrum f...   \n",
       "...                                                  ...   \n",
       "58673  data used in the preparation of this article w...   \n",
       "58674  interestingly the genome sequences of sars cov...   \n",
       "58675  as part of the program for international stude...   \n",
       "58676  analysis considered first time beginning posts...   \n",
       "58677  my prior research illustrated with use of begi...   \n",
       "\n",
       "                                                   kword  \\\n",
       "0      ['program for the international assessment of ...   \n",
       "1      ['trends in international mathematics and scie...   \n",
       "2           ['agricultural resources management survey']   \n",
       "3           ['agricultural resources management survey']   \n",
       "4                  ['farm income and wealth statistics']   \n",
       "...                                                  ...   \n",
       "58673  ['alzheimer s disease neuroimaging initiative ...   \n",
       "58674                 ['genome sequences of sars cov 2']   \n",
       "58675  ['trends in international mathematics and scie...   \n",
       "58676               ['beginning postsecondary students']   \n",
       "58677                ['beginning postsecondary student']   \n",
       "\n",
       "                                                   label  sent_len  \n",
       "0      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        45  \n",
       "1      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        94  \n",
       "2      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...        26  \n",
       "3      ['O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', ...        29  \n",
       "4      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        19  \n",
       "...                                                  ...       ...  \n",
       "58673  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        23  \n",
       "58674  ['O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'B', ...        38  \n",
       "58675  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        66  \n",
       "58676  ['O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'O', ...        37  \n",
       "58677  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...       105  \n",
       "\n",
       "[58678 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_df = pd.read_csv(\"../labelled_data/unique_train_df_5_len_128_cleaned_extra_labels_Albert.csv\")\n",
    "unique_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping all the sentences less than 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>train_sentences</th>\n",
       "      <th>kword</th>\n",
       "      <th>label</th>\n",
       "      <th>sent_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007f880-0a9b-492d-9a58-76eb0b0e0bd7</td>\n",
       "      <td>in fact organizations are now identifying digi...</td>\n",
       "      <td>['program for the international assessment of ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008656f-0ba2-4632-8602-3017b44c2e90</td>\n",
       "      <td>besides not enough young people are entering s...</td>\n",
       "      <td>['trends in international mathematics and scie...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>1 manages access to results of the agricultura...</td>\n",
       "      <td>['agricultural resources management survey']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>the agricultural resources management survey a...</td>\n",
       "      <td>['agricultural resources management survey']</td>\n",
       "      <td>['O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>the resulting statistics provide the fulcrum f...</td>\n",
       "      <td>['farm income and wealth statistics']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58673</th>\n",
       "      <td>ffd4d86a-0f26-44cc-baed-f0e209cc22af</td>\n",
       "      <td>data used in the preparation of this article w...</td>\n",
       "      <td>['alzheimer s disease neuroimaging initiative ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58674</th>\n",
       "      <td>ffe7f334-245a-4de7-b600-d7ff4e28bfca</td>\n",
       "      <td>interestingly the genome sequences of sars cov...</td>\n",
       "      <td>['genome sequences of sars cov 2']</td>\n",
       "      <td>['O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'B', ...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58675</th>\n",
       "      <td>ffeb3568-7aed-4dbe-b177-cbd7f46f34af</td>\n",
       "      <td>as part of the program for international stude...</td>\n",
       "      <td>['trends in international mathematics and scie...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58676</th>\n",
       "      <td>ffee2676-a778-4521-b947-e1e420b126c5</td>\n",
       "      <td>analysis considered first time beginning posts...</td>\n",
       "      <td>['beginning postsecondary students']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'O', ...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58677</th>\n",
       "      <td>ffee2676-a778-4521-b947-e1e420b126c5</td>\n",
       "      <td>my prior research illustrated with use of begi...</td>\n",
       "      <td>['beginning postsecondary student']</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58678 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  \\\n",
       "0      0007f880-0a9b-492d-9a58-76eb0b0e0bd7   \n",
       "1      0008656f-0ba2-4632-8602-3017b44c2e90   \n",
       "2      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "3      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "4      000e04d6-d6ef-442f-b070-4309493221ba   \n",
       "...                                     ...   \n",
       "58673  ffd4d86a-0f26-44cc-baed-f0e209cc22af   \n",
       "58674  ffe7f334-245a-4de7-b600-d7ff4e28bfca   \n",
       "58675  ffeb3568-7aed-4dbe-b177-cbd7f46f34af   \n",
       "58676  ffee2676-a778-4521-b947-e1e420b126c5   \n",
       "58677  ffee2676-a778-4521-b947-e1e420b126c5   \n",
       "\n",
       "                                         train_sentences  \\\n",
       "0      in fact organizations are now identifying digi...   \n",
       "1      besides not enough young people are entering s...   \n",
       "2      1 manages access to results of the agricultura...   \n",
       "3      the agricultural resources management survey a...   \n",
       "4      the resulting statistics provide the fulcrum f...   \n",
       "...                                                  ...   \n",
       "58673  data used in the preparation of this article w...   \n",
       "58674  interestingly the genome sequences of sars cov...   \n",
       "58675  as part of the program for international stude...   \n",
       "58676  analysis considered first time beginning posts...   \n",
       "58677  my prior research illustrated with use of begi...   \n",
       "\n",
       "                                                   kword  \\\n",
       "0      ['program for the international assessment of ...   \n",
       "1      ['trends in international mathematics and scie...   \n",
       "2           ['agricultural resources management survey']   \n",
       "3           ['agricultural resources management survey']   \n",
       "4                  ['farm income and wealth statistics']   \n",
       "...                                                  ...   \n",
       "58673  ['alzheimer s disease neuroimaging initiative ...   \n",
       "58674                 ['genome sequences of sars cov 2']   \n",
       "58675  ['trends in international mathematics and scie...   \n",
       "58676               ['beginning postsecondary students']   \n",
       "58677                ['beginning postsecondary student']   \n",
       "\n",
       "                                                   label  sent_len  \n",
       "0      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        45  \n",
       "1      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        94  \n",
       "2      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...        26  \n",
       "3      ['O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', ...        29  \n",
       "4      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        19  \n",
       "...                                                  ...       ...  \n",
       "58673  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        23  \n",
       "58674  ['O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'B', ...        38  \n",
       "58675  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...        66  \n",
       "58676  ['O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'O', ...        37  \n",
       "58677  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', ...       105  \n",
       "\n",
       "[58678 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_df = unique_df.loc[unique_df.sent_len<512, :].reset_index(drop=True)\n",
    "unique_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take unqiue of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56987, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_df = unique_df.drop_duplicates()\n",
    "unique_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking the sample of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56987, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique_df = unique_df.sample(int(unique_df.shape[0]*0.5)).reset_index(drop=True)\n",
    "unique_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54137, 5) (2850, 5)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "train_df, valid_df = train_test_split(unique_df, test_size=0.05)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape, valid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the DataFrame back to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_2_idx = {'O': 0 , 'B': 1, 'P': 2}\n",
    "\n",
    "def dataset_2_list(df):\n",
    "    id_list = df.id.values.tolist()\n",
    "    sentences_list = df.train_sentences.values.tolist()\n",
    "    keywords_list = df.kword.apply(lambda x : eval(x)).values.tolist()\n",
    "    \n",
    "    labels_list = df.label.apply(lambda x : eval(x)).values.tolist()    \n",
    "    labels_list = [list(map(tags_2_idx.get, lab)) for lab in labels_list]\n",
    "    \n",
    "    return id_list, sentences_list, keywords_list, labels_list\n",
    "\n",
    "final_train_id_list, final_train_sentences, final_train_keywords, final_train_labels = dataset_2_list(df=unique_df)\n",
    "final_valid_id_list, final_valid_sentences, final_valid_keywords, final_valid_labels = dataset_2_list(df=valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # verification\n",
    "# ind = 800\n",
    "# final_train_sentences[ind], config['tokenizer'].tokenize(final_train_sentences[ind]), final_train_keywords[ind], final_train_labels[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forming the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class form_input():\n",
    "    def __init__(self, ID, sentence, kword, label, data_type='test'):\n",
    "        self.id = ID\n",
    "        self.sentence = sentence\n",
    "        self.kword = kword\n",
    "        self.label = label\n",
    "        self.max_length = config['MAX_LEN']\n",
    "        self.tokenizer = config['tokenizer']\n",
    "        self.data_type = data_type\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        toks = config['tokenizer'].tokenize(self.sentence[item])\n",
    "        \n",
    "        if len(toks)>self.max_length:\n",
    "            toks = toks[:self.max_length]\n",
    "            \n",
    "        ########################################\n",
    "        # Forming the inputs\n",
    "        ids = config['tokenizer'].convert_tokens_to_ids(toks)\n",
    "        tok_type_id = [0] * len(ids)\n",
    "        att_mask = [1] * len(ids)\n",
    "        \n",
    "        # Padding\n",
    "        pad_len = self.max_length - len(ids)        \n",
    "        ids = ids + [2] * pad_len\n",
    "        tok_type_id = tok_type_id + [0] * pad_len\n",
    "        att_mask = att_mask + [0] * pad_len\n",
    "        \n",
    "        ########################################\n",
    "        # Forming the label\n",
    "        if self.data_type != 'test':\n",
    "            label = self.label[item] \n",
    "            if len(label)>self.max_length:\n",
    "                label = label[:self.max_length]\n",
    "            else:\n",
    "                label = label + [2] * pad_len\n",
    "\n",
    "        else:\n",
    "            label = 1\n",
    "        ########################################\n",
    "        \n",
    "#         print(item, len(ids), len(tok_type_id), len(att_mask), len(label))\n",
    "        \n",
    "        return {'pub_id': self.id[item],\n",
    "                'item': item,\n",
    "                #'sentence': self.sentence[item],\n",
    "                #'kword' : self.kword[item],\n",
    "                'ids': torch.tensor(ids, dtype = torch.long),\n",
    "                'tok_type_id': torch.tensor(tok_type_id, dtype = torch.long),\n",
    "                'att_mask': torch.tensor(att_mask, dtype = torch.long),\n",
    "                'target': torch.tensor(label, dtype = torch.long)\n",
    "               }\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod_input = form_input(ID=final_train_id_list, \n",
    "                              sentence=final_train_sentences, \n",
    "                              kword=final_train_keywords, \n",
    "                              label=final_train_labels, \n",
    "                              data_type='train')\n",
    "\n",
    "valid_prod_input = form_input(ID=final_valid_id_list, \n",
    "                              sentence=final_valid_sentences, \n",
    "                              kword=final_valid_keywords, \n",
    "                              label=final_valid_labels, \n",
    "                              data_type='valid')\n",
    "\n",
    "train_prod_input_data_loader = DataLoader(train_prod_input, \n",
    "                                          batch_size= config['batch_size'], \n",
    "                                          shuffle=True)\n",
    "\n",
    "valid_prod_input_data_loader = DataLoader(valid_prod_input, \n",
    "                                          batch_size= config['batch_size'], \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ind = 8443\n",
    "# train_prod_input[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 128, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 360\n",
    "len(train_prod_input[ind]['ids']), len(train_prod_input[ind]['att_mask']), len(train_prod_input[ind]['tok_type_id']), len(train_prod_input[ind]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    actual_flat = labels.flatten()\n",
    "    pred_flat = preds.flatten()\n",
    "    \n",
    "    # Get all non-padded index\n",
    "    non_padded_index = np.where(actual_flat != tags_2_idx[\"P\"])[0]\n",
    "    \n",
    "    # Only non-padded positions for the actual and the predicted\n",
    "    actual_flat = actual_flat[non_padded_index]\n",
    "    pred_flat = pred_flat[non_padded_index]\n",
    "    \n",
    "    print(confusion_matrix(actual_flat, pred_flat))\n",
    "    \n",
    "    # Accuracy\n",
    "    Accuracy = np.sum(actual_flat == pred_flat ) / len(pred_flat )\n",
    "    \n",
    "    # Recall calculator\n",
    "    condition = (actual_flat == 1)\n",
    "    actual_index = np.where(condition)[0]\n",
    "    actual_flat_rec = actual_flat[actual_index]\n",
    "    pred_flat_rec = pred_flat[actual_index]\n",
    "    Recall = np.sum(actual_flat_rec == pred_flat_rec) / len(pred_flat_rec)\n",
    "    \n",
    "    # Precision calculator\n",
    "    condition = (pred_flat == 1)\n",
    "    pred_index = np.where(condition)[0]\n",
    "    actual_flat_prc = actual_flat[pred_index]\n",
    "    pred_flat_prc = pred_flat[pred_index]\n",
    "    Precision = np.sum(actual_flat_prc == pred_flat_prc) / len(pred_flat_prc)\n",
    "\n",
    "    # Jaccard similarity\n",
    "    common_index = np.intersect1d(actual_index, pred_index)\n",
    "    jaccard_similarity = float(len(common_index))/( len(actual_index) + len(pred_index) - len(common_index))\n",
    "    \n",
    "    return Accuracy, Recall, Precision, jaccard_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for index, dataset in enumerate(tqdm(data_loader, total = len(data_loader))):\n",
    "        batch_input_ids = dataset['ids'].to(config['device'], dtype = torch.long)\n",
    "        batch_att_mask = dataset['att_mask'].to(config['device'], dtype = torch.long)\n",
    "        batch_tok_type_id = dataset['tok_type_id'].to(config['device'], dtype = torch.long)\n",
    "        batch_target = dataset['target'].to(config['device'], dtype = torch.long)\n",
    "                \n",
    "        model.zero_grad()\n",
    "        output = model(batch_input_ids, \n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=batch_att_mask,\n",
    "                       labels=batch_target)\n",
    "        \n",
    "        step_loss = output[0]\n",
    "        prediction = output[1]\n",
    "        \n",
    "        step_loss.sum().backward()\n",
    "        optimizer.step()        \n",
    "        train_loss += step_loss\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    tr_loss = train_loss.sum()/len(data_loader)\n",
    "    \n",
    "    return tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    predictions = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])\n",
    "    true_labels = np.array([], dtype = np.int64).reshape(0, config['MAX_LEN'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, dataset in enumerate(tqdm(data_loader, total = len(data_loader))):\n",
    "            batch_input_ids = dataset['ids'].to(config['device'], dtype = torch.long)\n",
    "            batch_att_mask = dataset['att_mask'].to(config['device'], dtype = torch.long)\n",
    "            batch_tok_type_id = dataset['tok_type_id'].to(config['device'], dtype = torch.long)\n",
    "            batch_target = dataset['target'].to(config['device'], dtype = torch.long)\n",
    "\n",
    "            output = model(batch_input_ids, \n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=batch_att_mask,\n",
    "                           labels=batch_target)\n",
    "\n",
    "            step_loss = output[0]\n",
    "            eval_prediction = output[1]\n",
    "\n",
    "            eval_loss += step_loss\n",
    "            \n",
    "            eval_prediction = np.argmax(eval_prediction.detach().to('cpu').numpy(), axis = 2)\n",
    "            actual = batch_target.to('cpu').numpy()\n",
    "            \n",
    "            predictions = np.concatenate((predictions, eval_prediction), axis = 0)\n",
    "            true_labels = np.concatenate((true_labels, actual), axis = 0)\n",
    "           \n",
    "        ev_loss = eval_loss.sum()/len(data_loader)\n",
    "        Accuracy, Recall, Precision, jaccard_similarity = flat_accuracy(preds=predictions , labels=true_labels)\n",
    "        \n",
    "        print(f'Eval Accuracy: {Accuracy}, Recall: {Recall}, Precision: {Precision}, jaccard_similarity: {jaccard_similarity}')\n",
    "            \n",
    "    return ev_loss, Precision, predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_engine(epoch, train_data, valid_data):\n",
    "    model = transformers.AlbertForTokenClassification.from_pretrained('albert-base-v2',  num_labels = len(tags_2_idx))\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(config['device'])\n",
    "    \n",
    "    params = model.parameters()\n",
    "    optimizer = torch.optim.Adam(params, lr= 3e-5)\n",
    "    \n",
    "    best_eval_loss = 1000000\n",
    "    best_Precision = 0\n",
    "    for i in range(epoch):\n",
    "        train_loss = train_fn(data_loader = train_data, \n",
    "                              model=model, \n",
    "                              optimizer=optimizer)\n",
    "        eval_loss, Precision, eval_predictions, true_labels = eval_fn(data_loader = valid_data, \n",
    "                                                                      model=model)\n",
    "        \n",
    "        print(f\"Epoch {i} , Train loss: {train_loss}, Eval loss: {eval_loss}\")\n",
    "\n",
    "#         if Precision > best_Precision:\n",
    "#             best_Precision = Precision           \n",
    "        \n",
    "        saving_name = config['model_path'] + f\"epoch_{i}_\" + config['model_name']\n",
    "        print(\"Saving the model:\", saving_name)\n",
    "        torch.save(model.state_dict(), saving_name)\n",
    "            \n",
    "    return model, eval_predictions, true_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForTokenClassification: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias']\n",
      "- This IS expected if you are initializing AlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 891/891 [06:54<00:00,  2.15it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:08<00:00,  5.59it/s]\n",
      "  0%|          | 0/891 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107639      8]\n",
      " [ 14844     19]]\n",
      "Eval Accuracy: 0.8787690800750959, Recall: 0.0012783421920204534, Precision: 0.7037037037037037, jaccard_similarity: 0.0012776544953264743\n",
      "Epoch 0 , Train loss: 0.3496091961860657, Eval loss: 0.34251123666763306\n",
      "Saving the model: ../output/epoch_0_albert_base_uncased_cleaned_extra_label_100per_data.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 891/891 [06:55<00:00,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:08<00:00,  5.50it/s]\n",
      "  0%|          | 0/891 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107376    271]\n",
      " [   182  14681]]\n",
      "Eval Accuracy: 0.9963023426659048, Recall: 0.987754827423804, Precision: 0.9818753344034243, jaccard_similarity: 0.9700673979119863\n",
      "Epoch 1 , Train loss: 0.04204551503062248, Eval loss: 0.01248400378972292\n",
      "Saving the model: ../output/epoch_1_albert_base_uncased_cleaned_extra_label_100per_data.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 891/891 [06:55<00:00,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:08<00:00,  5.55it/s]\n",
      "  0%|          | 0/891 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107468    179]\n",
      " [   114  14749]]\n",
      "Eval Accuracy: 0.9976083585013469, Recall: 0.9923299468478772, Precision: 0.9880091103965702, jaccard_similarity: 0.9805212072862651\n",
      "Epoch 2 , Train loss: 0.010658901184797287, Eval loss: 0.007601642981171608\n",
      "Saving the model: ../output/epoch_2_albert_base_uncased_cleaned_extra_label_100per_data.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 891/891 [06:55<00:00,  2.14it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:08<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107489    158]\n",
      " [   131  14732]]\n",
      "Eval Accuracy: 0.9976410088972328, Recall: 0.991186166991859, Precision: 0.9893888515782404, jaccard_similarity: 0.9807602689567938\n",
      "Epoch 3 , Train loss: 0.008217979222536087, Eval loss: 0.007283493876457214\n",
      "Saving the model: ../output/epoch_3_albert_base_uncased_cleaned_extra_label_100per_data.bin\n"
     ]
    }
   ],
   "source": [
    "model, eval_predictions, eval_true_labels = train_engine(epoch=config['Epoch'], \n",
    "                                                         train_data=train_prod_input_data_loader, \n",
    "                                                         valid_data=valid_prod_input_data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
