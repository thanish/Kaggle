{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:12.107791Z",
     "iopub.status.busy": "2021-07-23T11:52:12.107138Z",
     "iopub.status.idle": "2021-07-23T11:52:12.108858Z",
     "shell.execute_reply": "2021-07-23T11:52:12.108348Z",
     "shell.execute_reply.started": "2021-07-23T02:57:25.059291Z"
    },
    "papermill": {
     "duration": 0.028433,
     "end_time": "2021-07-23T11:52:12.108985",
     "exception": false,
     "start_time": "2021-07-23T11:52:12.080552",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:12.222273Z",
     "iopub.status.busy": "2021-07-23T11:52:12.221464Z",
     "iopub.status.idle": "2021-07-23T11:52:19.075157Z",
     "shell.execute_reply": "2021-07-23T11:52:19.074634Z",
     "shell.execute_reply.started": "2021-07-23T10:49:21.522771Z"
    },
    "papermill": {
     "duration": 6.951053,
     "end_time": "2021-07-23T11:52:19.075291",
     "exception": false,
     "start_time": "2021-07-23T11:52:12.124238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from transformers import BertModel, RobertaModel, AutoTokenizer, AutoConfig, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from torch.optim import AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.155479Z",
     "iopub.status.busy": "2021-07-23T11:52:19.154783Z",
     "iopub.status.idle": "2021-07-23T11:52:19.320689Z",
     "shell.execute_reply": "2021-07-23T11:52:19.319760Z",
     "shell.execute_reply.started": "2021-07-23T10:49:27.944862Z"
    },
    "papermill": {
     "duration": 0.229699,
     "end_time": "2021-07-23T11:52:19.320880",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.091181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "platform = 'Kaggle'\n",
    "train_type = 'val'\n",
    "model_2_train = 'roberta-large'\n",
    "\n",
    "model_suffix = 'Roberta_large_model.bin'\n",
    "\n",
    "if platform=='Kaggle':\n",
    "    train_path = \"../input/commonlitmodels/train_5_fold_CV.csv\"\n",
    "    test_path = \"../input/commonlitreadabilityprize/test.csv\"\n",
    "    bert_path = '../input/huggingface-bert/bert-base-uncased'\n",
    "    roberta_base_path = '../input/huggingface-roberta/roberta-base'\n",
    "    roberta_large_path = '../input/huggingface-roberta/roberta-large'\n",
    "    model_dir = './'\n",
    "else:\n",
    "    train_path = '../data/train_5_fold_CV.csv'\n",
    "    test_path =  '../data/test.csv'\n",
    "    roberta_base_path = 'roberta-base'\n",
    "    roberta_large_path = 'roberta-large'\n",
    "    model_dir = \"../models/\"\n",
    "    \n",
    "    \n",
    "HIDDEN_SIZE = 1024\n",
    "config = {'train_path':train_path, \n",
    "          'test_path':test_path, \n",
    "          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "          'tokenizer': AutoTokenizer.from_pretrained(roberta_large_path, lower=True), \n",
    "          'model_dir': model_dir,\n",
    "          'model_suffix':model_suffix\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.357727Z",
     "iopub.status.busy": "2021-07-23T11:52:19.356944Z",
     "iopub.status.idle": "2021-07-23T11:52:19.359439Z",
     "shell.execute_reply": "2021-07-23T11:52:19.359007Z",
     "shell.execute_reply.started": "2021-07-23T10:49:28.164758Z"
    },
    "papermill": {
     "duration": 0.022002,
     "end_time": "2021-07-23T11:52:19.359543",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.337541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    \n",
    "    temp = pd.read_csv(path)\n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.395107Z",
     "iopub.status.busy": "2021-07-23T11:52:19.394489Z",
     "iopub.status.idle": "2021-07-23T11:52:19.564103Z",
     "shell.execute_reply": "2021-07-23T11:52:19.563542Z",
     "shell.execute_reply.started": "2021-07-23T10:49:28.171669Z"
    },
    "papermill": {
     "duration": 0.189606,
     "end_time": "2021-07-23T11:52:19.564249",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.374643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>tb_fold</th>\n",
       "      <th>abt_fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4f53dd071</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Astronomy</td>\n",
       "      <td>CC BY-SA 3.0 and GFDL</td>\n",
       "      <td>Astronomy is a natural science. It is the stud...</td>\n",
       "      <td>-0.123139</td>\n",
       "      <td>0.537258</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29cd28197</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Light-year</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The light-year is a unit of length used to exp...</td>\n",
       "      <td>-3.256312</td>\n",
       "      <td>0.581264</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>924cdefd9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>He flung his pick out of the trench, climbed o...</td>\n",
       "      <td>-1.100205</td>\n",
       "      <td>0.487602</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8d406094c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am sorry to hear that Bulgaria demands conce...</td>\n",
       "      <td>-1.565674</td>\n",
       "      <td>0.504832</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3c1674b21</td>\n",
       "      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n",
       "      <td>CC BY 4.0</td>\n",
       "      <td>To practice their knowledge and skills, doctor...</td>\n",
       "      <td>-0.878021</td>\n",
       "      <td>0.446454</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>1c9fe933e</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Between his apprehension and his trial no frie...</td>\n",
       "      <td>-1.999922</td>\n",
       "      <td>0.463280</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>6b9d9517c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Among the most subtle and skillful of all the ...</td>\n",
       "      <td>-1.877338</td>\n",
       "      <td>0.452149</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>ebac871b7</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Lithosphere</td>\n",
       "      <td>CC BY-SA 3.0 and GFDL</td>\n",
       "      <td>There are two types of lithosphere:\\nOceanic l...</td>\n",
       "      <td>-2.366802</td>\n",
       "      <td>0.498877</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>84fba32e2</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Moon</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The Moon is Earth's only permanent natural sat...</td>\n",
       "      <td>-0.518585</td>\n",
       "      <td>0.494256</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>14972af78</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Artificial_muscle</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Artificial muscle is a generic term used for m...</td>\n",
       "      <td>-1.673455</td>\n",
       "      <td>0.471585</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2834 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     4f53dd071        https://simple.wikipedia.org/wiki/Astronomy   \n",
       "1     29cd28197           https://en.wikipedia.org/wiki/Light-year   \n",
       "2     924cdefd9                                                NaN   \n",
       "3     8d406094c                                                NaN   \n",
       "4     3c1674b21  https://kids.frontiersin.org/article/10.3389/f...   \n",
       "...         ...                                                ...   \n",
       "2829  1c9fe933e                                                NaN   \n",
       "2830  6b9d9517c                                                NaN   \n",
       "2831  ebac871b7      https://simple.wikipedia.org/wiki/Lithosphere   \n",
       "2832  84fba32e2                 https://en.wikipedia.org/wiki/Moon   \n",
       "2833  14972af78    https://en.wikipedia.org/wiki/Artificial_muscle   \n",
       "\n",
       "                    license  \\\n",
       "0     CC BY-SA 3.0 and GFDL   \n",
       "1              CC BY-SA 3.0   \n",
       "2                       NaN   \n",
       "3                       NaN   \n",
       "4                 CC BY 4.0   \n",
       "...                     ...   \n",
       "2829                    NaN   \n",
       "2830                    NaN   \n",
       "2831  CC BY-SA 3.0 and GFDL   \n",
       "2832           CC BY-SA 3.0   \n",
       "2833           CC BY-SA 3.0   \n",
       "\n",
       "                                                excerpt    target  \\\n",
       "0     Astronomy is a natural science. It is the stud... -0.123139   \n",
       "1     The light-year is a unit of length used to exp... -3.256312   \n",
       "2     He flung his pick out of the trench, climbed o... -1.100205   \n",
       "3     I am sorry to hear that Bulgaria demands conce... -1.565674   \n",
       "4     To practice their knowledge and skills, doctor... -0.878021   \n",
       "...                                                 ...       ...   \n",
       "2829  Between his apprehension and his trial no frie... -1.999922   \n",
       "2830  Among the most subtle and skillful of all the ... -1.877338   \n",
       "2831  There are two types of lithosphere:\\nOceanic l... -2.366802   \n",
       "2832  The Moon is Earth's only permanent natural sat... -0.518585   \n",
       "2833  Artificial muscle is a generic term used for m... -1.673455   \n",
       "\n",
       "      standard_error  tb_fold  abt_fold  \n",
       "0           0.537258        4         1  \n",
       "1           0.581264        4         1  \n",
       "2           0.487602        3         1  \n",
       "3           0.504832        3         1  \n",
       "4           0.446454        2         1  \n",
       "...              ...      ...       ...  \n",
       "2829        0.463280        2         5  \n",
       "2830        0.452149        1         5  \n",
       "2831        0.498877        2         5  \n",
       "2832        0.494256        4         5  \n",
       "2833        0.471585        5         5  \n",
       "\n",
       "[2834 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = read_dataset(path=config['train_path'])\n",
    "test_df = read_dataset(path=config['test_path'])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.607000Z",
     "iopub.status.busy": "2021-07-23T11:52:19.606157Z",
     "iopub.status.idle": "2021-07-23T11:52:19.609203Z",
     "shell.execute_reply": "2021-07-23T11:52:19.608610Z",
     "shell.execute_reply.started": "2021-07-23T10:49:28.351964Z"
    },
    "papermill": {
     "duration": 0.024113,
     "end_time": "2021-07-23T11:52:19.609320",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.585207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df = train_df.sample(200).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.692533Z",
     "iopub.status.busy": "2021-07-23T11:52:19.691590Z",
     "iopub.status.idle": "2021-07-23T11:52:19.693752Z",
     "shell.execute_reply": "2021-07-23T11:52:19.694202Z",
     "shell.execute_reply.started": "2021-07-23T10:49:28.381164Z"
    },
    "papermill": {
     "duration": 0.029631,
     "end_time": "2021-07-23T11:52:19.694316",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.664685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MeanPoolingModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(roberta_large_path)\n",
    "        self.model = AutoModel.from_pretrained(roberta_large_path, config=config)\n",
    "        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        \n",
    "        outputs = self.model(ids, mask)\n",
    "        last_hidden_state = outputs[0]\n",
    "        input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        logits = self.linear(mean_embeddings)\n",
    "        \n",
    "        preds = logits.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "class ConcatenateLastFourModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(roberta_large_path)\n",
    "        config.update({'output_hidden_states':True})\n",
    "        self.model = AutoModel.from_pretrained(roberta_large_path, config=config)\n",
    "        self.linear = nn.Linear(4*HIDDEN_SIZE, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        \n",
    "        outputs = self.model(ids, mask)\n",
    "        all_hidden_states = torch.stack(outputs[2])\n",
    "        concatenate_pooling = torch.cat(\n",
    "            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), -1\n",
    "        )\n",
    "        concatenate_pooling = concatenate_pooling[:,0]\n",
    "        logits = self.linear(concatenate_pooling)\n",
    "        \n",
    "        preds = logits.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.740790Z",
     "iopub.status.busy": "2021-07-23T11:52:19.739908Z",
     "iopub.status.idle": "2021-07-23T11:52:19.742921Z",
     "shell.execute_reply": "2021-07-23T11:52:19.742407Z",
     "shell.execute_reply.started": "2021-07-23T10:49:28.410497Z"
    },
    "papermill": {
     "duration": 0.030882,
     "end_time": "2021-07-23T11:52:19.743038",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.712156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class form_input():\n",
    "    def __init__(self, row_ID, sentence, target, data_type):\n",
    "        self.row_ID = row_ID\n",
    "        self.sentence=sentence\n",
    "        self.target = target\n",
    "        self.data_type=data_type\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        toks = config['tokenizer'].encode_plus(self.sentence[item])\n",
    "        \n",
    "        ids = toks['input_ids']\n",
    "        att_mask = toks['attention_mask']\n",
    "        label = self.target[item]\n",
    "        \n",
    "        pad_len = params_dict['Max_length'] - len(ids)\n",
    "        \n",
    "        # Padding.\n",
    "        if len(ids)>params_dict['Max_length']:\n",
    "            ids = ids[:params_dict['Max_length']]\n",
    "            att_mask = att_mask[:params_dict['Max_length']]\n",
    "        else:\n",
    "            ids = ids + [2]*pad_len\n",
    "            att_mask = att_mask + [0]*pad_len\n",
    "            \n",
    "        ################################################\n",
    "        if ((self.data_type=='train') | (self.data_type=='valid')):\n",
    "            target = self.target[item]\n",
    "        else:\n",
    "            target = 1\n",
    "                    \n",
    "        return {#'row_ID': torch.tensor(self.row_ID[item], dtype=torch.long), \n",
    "#                 'sentence': torch.tensor(self.sentence[item], dtype=torch.long), \n",
    "                'ids': torch.tensor(ids, dtype=torch.long), \n",
    "                'att_mask': torch.tensor(att_mask, dtype=torch.long), \n",
    "                'target': torch.tensor(target, dtype=torch.float) }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.781586Z",
     "iopub.status.busy": "2021-07-23T11:52:19.780881Z",
     "iopub.status.idle": "2021-07-23T11:52:19.783459Z",
     "shell.execute_reply": "2021-07-23T11:52:19.783061Z",
     "shell.execute_reply.started": "2021-07-23T10:49:28.431676Z"
    },
    "papermill": {
     "duration": 0.022925,
     "end_time": "2021-07-23T11:52:19.783559",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.760634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input(df, data_type):\n",
    "    temp = form_input(row_ID=df.id,\n",
    "                      sentence=df.excerpt, \n",
    "                      target=df.target, \n",
    "                      data_type=data_type)\n",
    "    \n",
    "    return temp\n",
    "    \n",
    "\n",
    "def get_data_loader(class_input, batch_size, shuffle):\n",
    "    temp_data_loader = DataLoader(class_input, \n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle\n",
    "                                 )\n",
    "    \n",
    "    return temp_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.821657Z",
     "iopub.status.busy": "2021-07-23T11:52:19.820954Z",
     "iopub.status.idle": "2021-07-23T11:52:19.823502Z",
     "shell.execute_reply": "2021-07-23T11:52:19.823012Z",
     "shell.execute_reply.started": "2021-07-23T10:49:28.684059Z"
    },
    "papermill": {
     "duration": 0.02296,
     "end_time": "2021-07-23T11:52:19.823599",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.800639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.863335Z",
     "iopub.status.busy": "2021-07-23T11:52:19.862501Z",
     "iopub.status.idle": "2021-07-23T11:52:19.864906Z",
     "shell.execute_reply": "2021-07-23T11:52:19.865376Z",
     "shell.execute_reply.started": "2021-07-23T10:49:29.203123Z"
    },
    "papermill": {
     "duration": 0.025003,
     "end_time": "2021-07-23T11:52:19.865504",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.840501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.909053Z",
     "iopub.status.busy": "2021-07-23T11:52:19.908099Z",
     "iopub.status.idle": "2021-07-23T11:52:19.910977Z",
     "shell.execute_reply": "2021-07-23T11:52:19.910462Z",
     "shell.execute_reply.started": "2021-07-23T10:49:29.628252Z"
    },
    "papermill": {
     "duration": 0.027291,
     "end_time": "2021-07-23T11:52:19.911145",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.883854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    \n",
    "    roberta_parameters = named_parameters[:388]    \n",
    "    attention_parameters = named_parameters[391:395]\n",
    "    regressor_parameters = named_parameters[395:]\n",
    "        \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "\n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "\n",
    "        lr = 2e-5\n",
    "\n",
    "        if layer_num >= 69:        \n",
    "            lr = 2e-5\n",
    "\n",
    "        if layer_num >= 133:\n",
    "            lr = 2e-5\n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "\n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:19.961656Z",
     "iopub.status.busy": "2021-07-23T11:52:19.960875Z",
     "iopub.status.idle": "2021-07-23T11:52:19.963422Z",
     "shell.execute_reply": "2021-07-23T11:52:19.962925Z",
     "shell.execute_reply.started": "2021-07-23T10:49:30.137922Z"
    },
    "papermill": {
     "duration": 0.03599,
     "end_time": "2021-07-23T11:52:19.963519",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.927529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(train_data_loader\n",
    "             , valid_data_loader\n",
    "             , model\n",
    "             , optimizer\n",
    "             , fold_no\n",
    "             , epoch\n",
    "             , best_score\n",
    "             , best_epoch\n",
    "             , best_step \n",
    "             , scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    eval_at_every = params_dict['eval_every_step']\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for current_step, data in enumerate(train_data_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_ids = data['ids'].to(config['device'], dtype = torch.long)\n",
    "        batch_att_mask = data['att_mask'].to(config['device'], dtype = torch.long)\n",
    "        batch_target = data['target'].to(config['device'], dtype = torch.float)\n",
    "                \n",
    "        output = model(ids=batch_ids, \n",
    "                       mask=batch_att_mask)\n",
    "        batch_prediction = output.flatten()\n",
    "        \n",
    "        batch_train_loss = criterion(batch_prediction, batch_target)\n",
    "        train_loss+=batch_train_loss.sum()\n",
    "        \n",
    "        batch_train_loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "            \n",
    "        # Deleting the intermediate variables\n",
    "        del output, batch_prediction, batch_train_loss\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        if current_step%eval_at_every == 0:\n",
    "            # Calculate train loss for each step\n",
    "            interim_avg_train_loss = torch.sqrt(train_loss/(current_step+1))\n",
    "        \n",
    "            # Get the Eval results\n",
    "            eval_loss, actual, predicted_output = eval_fn(data_loader=valid_data_loader,\n",
    "                                                          model=model)\n",
    "            \n",
    "            # Get the actual and predicted\n",
    "            actual = actual.detach().cpu().numpy()\n",
    "            predicted_output = predicted_output.detach().cpu().numpy()\n",
    "\n",
    "            # Calculate Eval RMSE\n",
    "            eval_rmse = np.sqrt(mean_squared_error(predicted_output, actual))\n",
    "\n",
    "            print(f\"Fold:{fold_no}/{params_dict['total_folds']} Epoch:{epoch}/{params_dict['epoch']} Step: {current_step}/{len(train_data_loader)}, Train_loss: {interim_avg_train_loss :0.4f}, Eval_loss:{eval_loss:0.4f}, Eval RMSE:{eval_rmse:0.4f}\")\n",
    "            \n",
    "            if eval_rmse<best_score:\n",
    "                print(f\"Eval RMSE improved from {best_score:0.4f} to {eval_rmse:0.4f}\")\n",
    "                best_score = eval_rmse\n",
    "                best_epoch = epoch\n",
    "                best_fold = fold_no\n",
    "                best_step = current_step\n",
    "                \n",
    "                # Saving the model\n",
    "                model_name = f\"{config['model_dir']}fold_{fold_no}_{config['model_suffix']}\"\n",
    "                print(f\"Saving the model {model_name}\")        \n",
    "                torch.save(model.state_dict(), model_name)\n",
    "            else:\n",
    "                print(f\"Eval RMSE did not improve from the {best_score:0.4f} from epoch:{best_epoch} step:{best_step}\")\n",
    "\n",
    "            print(\"\")\n",
    "                \n",
    "    avg_train_loss = torch.sqrt(train_loss/len(train_data_loader))\n",
    "    \n",
    "    # Deleting the intermediate variables\n",
    "    del train_loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "                  \n",
    "    return avg_train_loss, best_score, best_epoch, best_step\n",
    "        \n",
    "    \n",
    "def eval_fn(data_loader, model):\n",
    "    \n",
    "    model.eval()\n",
    "    actual = torch.tensor([]).to(config['device'])\n",
    "    predicted_output = torch.tensor([]).to(config['device'])\n",
    "    \n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            batch_ids = data['ids'].to(config['device'])\n",
    "            batch_att_mask = data['att_mask'].to(config['device'])\n",
    "            batch_target = data['target'].to(config['device'])\n",
    "            \n",
    "            output = model(ids=batch_ids, \n",
    "                           mask=batch_att_mask)\n",
    "            batch_prediction = output.flatten()\n",
    "            \n",
    "            batch_eval_loss = criterion(batch_prediction, batch_target)\n",
    "            eval_loss+=batch_eval_loss.sum()\n",
    "            \n",
    "            actual = torch.hstack([actual, batch_target])\n",
    "            predicted_output = torch.hstack([predicted_output,  batch_prediction])\n",
    "            \n",
    "    avg_eval_loss = torch.sqrt(eval_loss/len(data_loader))\n",
    "            \n",
    "    return avg_eval_loss, actual, predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:20.004335Z",
     "iopub.status.busy": "2021-07-23T11:52:20.003534Z",
     "iopub.status.idle": "2021-07-23T11:52:20.007669Z",
     "shell.execute_reply": "2021-07-23T11:52:20.007171Z",
     "shell.execute_reply.started": "2021-07-23T10:49:30.711805Z"
    },
    "papermill": {
     "duration": 0.028697,
     "end_time": "2021-07-23T11:52:20.007814",
     "exception": false,
     "start_time": "2021-07-23T11:52:19.979117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_engine(train_data_loader, eval_data_loader, fold_no):\n",
    "        \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    seed_everything(seed=100)\n",
    "    model = MeanPoolingModel()\n",
    "#     model = nn.DataParallel(model)\n",
    "    model.to(config['device'])\n",
    "    \n",
    "#     optimizer = create_optimizer(model)     \n",
    "    optimizer = AdamW(model.parameters(), lr=params_dict['learning_rate'], eps = params_dict['EPS'])\n",
    "    \n",
    "#     scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "#                                                 num_training_steps=params_dict['epoch'] * len(train_data_loader),\n",
    "#                                                 num_warmup_steps=50)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0, \n",
    "                                                num_training_steps=len(train_data_loader) * params_dict['epoch'])\n",
    "        \n",
    "    best_score = 100000\n",
    "    best_epoch=0\n",
    "    best_step=0\n",
    "    for epoch in range(1, (params_dict['epoch']+1)):\n",
    "        train_loss, best_score, best_epoch, best_step = train_fn(train_data_loader=train_data_loader,\n",
    "                                                                 valid_data_loader=eval_data_loader,\n",
    "                                                                 model=model,\n",
    "                                                                 optimizer=optimizer,\n",
    "                                                                 epoch=epoch,\n",
    "                                                                 fold_no=fold_no,\n",
    "                                                                 best_score=best_score,\n",
    "                                                                 best_epoch=best_epoch,\n",
    "                                                                 best_step=best_step, \n",
    "                                                                 scheduler=scheduler)\n",
    "            \n",
    "        print(f\"--------------------------------------------------------------------------------\")\n",
    "        print(f\"----------------------Fold: {fold_no}, Epoch: {epoch} over----------------------\")\n",
    "        print(f\"--------------------------------------------------------------------------------\")\n",
    "        print(\"\")\n",
    "    \n",
    "    # Deleting the model and clearing the CUDA memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return  best_score, best_epoch, best_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:20.047715Z",
     "iopub.status.busy": "2021-07-23T11:52:20.047169Z",
     "iopub.status.idle": "2021-07-23T11:52:20.051377Z",
     "shell.execute_reply": "2021-07-23T11:52:20.050944Z",
     "shell.execute_reply.started": "2021-07-23T10:49:31.366264Z"
    },
    "papermill": {
     "duration": 0.027116,
     "end_time": "2021-07-23T11:52:20.051507",
     "exception": false,
     "start_time": "2021-07-23T11:52:20.024391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T11:52:20.260899Z",
     "iopub.status.busy": "2021-07-23T11:52:20.259973Z",
     "iopub.status.idle": "2021-07-23T15:17:30.905774Z",
     "shell.execute_reply": "2021-07-23T15:17:30.905287Z",
     "shell.execute_reply.started": "2021-07-23T10:49:31.940925Z"
    },
    "papermill": {
     "duration": 12310.839071,
     "end_time": "2021-07-23T15:17:30.906624",
     "exception": false,
     "start_time": "2021-07-23T11:52:20.067553",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Fold 1/5: Train fold: [3 2 5 4 1], Test fold:[4 3 5 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:1/5 Epoch:1/3 Step: 0/567, Train_loss: 1.5625, Eval_loss:1.0884, Eval RMSE:1.0953\n",
      "Eval RMSE improved from 100000.0000 to 1.0953\n",
      "Saving the model ./fold_1_Roberta_large_model.bin\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 20/567, Train_loss: 0.9596, Eval_loss:0.9698, Eval RMSE:0.9868\n",
      "Eval RMSE improved from 1.0953 to 0.9868\n",
      "Saving the model ./fold_1_Roberta_large_model.bin\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 40/567, Train_loss: 0.9023, Eval_loss:0.6711, Eval RMSE:0.6768\n",
      "Eval RMSE improved from 0.9868 to 0.6768\n",
      "Saving the model ./fold_1_Roberta_large_model.bin\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 60/567, Train_loss: 0.8825, Eval_loss:0.7728, Eval RMSE:0.7892\n",
      "Eval RMSE did not improve from the 0.6768 from epoch:1 step:40\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 80/567, Train_loss: 0.8405, Eval_loss:0.7360, Eval RMSE:0.7389\n",
      "Eval RMSE did not improve from the 0.6768 from epoch:1 step:40\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 100/567, Train_loss: 0.8417, Eval_loss:0.7053, Eval RMSE:0.7161\n",
      "Eval RMSE did not improve from the 0.6768 from epoch:1 step:40\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 120/567, Train_loss: 0.8236, Eval_loss:0.6381, Eval RMSE:0.6428\n",
      "Eval RMSE improved from 0.6768 to 0.6428\n",
      "Saving the model ./fold_1_Roberta_large_model.bin\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 140/567, Train_loss: 0.7919, Eval_loss:0.7574, Eval RMSE:0.7585\n",
      "Eval RMSE did not improve from the 0.6428 from epoch:1 step:120\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 160/567, Train_loss: 0.7937, Eval_loss:0.6512, Eval RMSE:0.6588\n",
      "Eval RMSE did not improve from the 0.6428 from epoch:1 step:120\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 180/567, Train_loss: 0.7785, Eval_loss:0.6596, Eval RMSE:0.6616\n",
      "Eval RMSE did not improve from the 0.6428 from epoch:1 step:120\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 200/567, Train_loss: 0.7741, Eval_loss:0.5851, Eval RMSE:0.5923\n",
      "Eval RMSE improved from 0.6428 to 0.5923\n",
      "Saving the model ./fold_1_Roberta_large_model.bin\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 220/567, Train_loss: 0.7757, Eval_loss:0.9459, Eval RMSE:0.9616\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 240/567, Train_loss: 0.7994, Eval_loss:1.0177, Eval RMSE:1.0336\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 260/567, Train_loss: 0.8139, Eval_loss:1.0329, Eval RMSE:1.0445\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 280/567, Train_loss: 0.8254, Eval_loss:1.0199, Eval RMSE:1.0366\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 300/567, Train_loss: 0.8431, Eval_loss:1.0316, Eval RMSE:1.0434\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 320/567, Train_loss: 0.8564, Eval_loss:1.0227, Eval RMSE:1.0399\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 340/567, Train_loss: 0.8765, Eval_loss:1.0879, Eval RMSE:1.0953\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 360/567, Train_loss: 0.8787, Eval_loss:1.0172, Eval RMSE:1.0323\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 380/567, Train_loss: 0.8901, Eval_loss:1.0343, Eval RMSE:1.0458\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 400/567, Train_loss: 0.8997, Eval_loss:1.0247, Eval RMSE:1.0422\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 420/567, Train_loss: 0.9032, Eval_loss:1.0200, Eval RMSE:1.0367\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 440/567, Train_loss: 0.9114, Eval_loss:1.0189, Eval RMSE:1.0330\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 460/567, Train_loss: 0.9128, Eval_loss:1.0172, Eval RMSE:1.0327\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 480/567, Train_loss: 0.9169, Eval_loss:1.0260, Eval RMSE:1.0436\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 500/567, Train_loss: 0.9272, Eval_loss:1.0181, Eval RMSE:1.0325\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 520/567, Train_loss: 0.9370, Eval_loss:1.0623, Eval RMSE:1.0825\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 540/567, Train_loss: 0.9435, Eval_loss:1.0191, Eval RMSE:1.0355\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:1/3 Step: 560/567, Train_loss: 0.9489, Eval_loss:1.0236, Eval RMSE:1.0409\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 1, Epoch: 1 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 0/567, Train_loss: 0.9571, Eval_loss:1.0259, Eval RMSE:1.0436\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 20/567, Train_loss: 1.1057, Eval_loss:1.0197, Eval RMSE:1.0335\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 40/567, Train_loss: 1.0480, Eval_loss:1.0178, Eval RMSE:1.0337\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 60/567, Train_loss: 1.0304, Eval_loss:1.0224, Eval RMSE:1.0395\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 80/567, Train_loss: 1.0207, Eval_loss:1.0370, Eval RMSE:1.0557\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 100/567, Train_loss: 1.0106, Eval_loss:1.0194, Eval RMSE:1.0359\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 120/567, Train_loss: 1.0177, Eval_loss:1.0294, Eval RMSE:1.0415\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 140/567, Train_loss: 1.0212, Eval_loss:1.0214, Eval RMSE:1.0348\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 160/567, Train_loss: 1.0267, Eval_loss:1.0269, Eval RMSE:1.0447\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 180/567, Train_loss: 1.0194, Eval_loss:1.0255, Eval RMSE:1.0382\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 200/567, Train_loss: 1.0236, Eval_loss:1.0174, Eval RMSE:1.0322\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 220/567, Train_loss: 1.0263, Eval_loss:1.0210, Eval RMSE:1.0379\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 240/567, Train_loss: 1.0226, Eval_loss:1.0197, Eval RMSE:1.0363\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 260/567, Train_loss: 1.0290, Eval_loss:1.0172, Eval RMSE:1.0325\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 280/567, Train_loss: 1.0244, Eval_loss:1.0172, Eval RMSE:1.0326\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 300/567, Train_loss: 1.0378, Eval_loss:1.0346, Eval RMSE:1.0460\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 320/567, Train_loss: 1.0336, Eval_loss:1.0185, Eval RMSE:1.0327\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 340/567, Train_loss: 1.0334, Eval_loss:1.0172, Eval RMSE:1.0323\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 360/567, Train_loss: 1.0355, Eval_loss:1.0330, Eval RMSE:1.0446\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 380/567, Train_loss: 1.0398, Eval_loss:1.0176, Eval RMSE:1.0334\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 400/567, Train_loss: 1.0359, Eval_loss:1.0191, Eval RMSE:1.0355\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 420/567, Train_loss: 1.0284, Eval_loss:1.0172, Eval RMSE:1.0326\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 440/567, Train_loss: 1.0287, Eval_loss:1.0197, Eval RMSE:1.0336\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 460/567, Train_loss: 1.0265, Eval_loss:1.0172, Eval RMSE:1.0327\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 480/567, Train_loss: 1.0294, Eval_loss:1.0173, Eval RMSE:1.0323\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 500/567, Train_loss: 1.0329, Eval_loss:1.0177, Eval RMSE:1.0323\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 520/567, Train_loss: 1.0352, Eval_loss:1.0193, Eval RMSE:1.0332\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 540/567, Train_loss: 1.0363, Eval_loss:1.0200, Eval RMSE:1.0338\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:2/3 Step: 560/567, Train_loss: 1.0386, Eval_loss:1.0182, Eval RMSE:1.0344\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 1, Epoch: 2 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 0/567, Train_loss: 1.3372, Eval_loss:1.0173, Eval RMSE:1.0329\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 20/567, Train_loss: 1.0880, Eval_loss:1.0173, Eval RMSE:1.0329\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 40/567, Train_loss: 1.1139, Eval_loss:1.0185, Eval RMSE:1.0348\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 60/567, Train_loss: 1.0794, Eval_loss:1.0172, Eval RMSE:1.0327\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 80/567, Train_loss: 1.0622, Eval_loss:1.0174, Eval RMSE:1.0322\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 100/567, Train_loss: 1.0738, Eval_loss:1.0179, Eval RMSE:1.0324\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 120/567, Train_loss: 1.0682, Eval_loss:1.0394, Eval RMSE:1.0504\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 140/567, Train_loss: 1.0668, Eval_loss:1.0172, Eval RMSE:1.0327\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 160/567, Train_loss: 1.0578, Eval_loss:1.0178, Eval RMSE:1.0324\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 180/567, Train_loss: 1.0519, Eval_loss:1.0204, Eval RMSE:1.0340\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 200/567, Train_loss: 1.0476, Eval_loss:1.0172, Eval RMSE:1.0324\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 220/567, Train_loss: 1.0499, Eval_loss:1.0179, Eval RMSE:1.0339\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 240/567, Train_loss: 1.0422, Eval_loss:1.0186, Eval RMSE:1.0348\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 260/567, Train_loss: 1.0429, Eval_loss:1.0185, Eval RMSE:1.0347\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 280/567, Train_loss: 1.0466, Eval_loss:1.0172, Eval RMSE:1.0324\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 300/567, Train_loss: 1.0408, Eval_loss:1.0186, Eval RMSE:1.0349\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 320/567, Train_loss: 1.0398, Eval_loss:1.0204, Eval RMSE:1.0372\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 340/567, Train_loss: 1.0412, Eval_loss:1.0180, Eval RMSE:1.0340\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 360/567, Train_loss: 1.0452, Eval_loss:1.0177, Eval RMSE:1.0336\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 380/567, Train_loss: 1.0440, Eval_loss:1.0172, Eval RMSE:1.0325\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 400/567, Train_loss: 1.0433, Eval_loss:1.0177, Eval RMSE:1.0323\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 420/567, Train_loss: 1.0424, Eval_loss:1.0189, Eval RMSE:1.0330\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 440/567, Train_loss: 1.0392, Eval_loss:1.0194, Eval RMSE:1.0333\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 460/567, Train_loss: 1.0379, Eval_loss:1.0177, Eval RMSE:1.0323\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 480/567, Train_loss: 1.0387, Eval_loss:1.0176, Eval RMSE:1.0323\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 500/567, Train_loss: 1.0349, Eval_loss:1.0174, Eval RMSE:1.0322\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 520/567, Train_loss: 1.0372, Eval_loss:1.0174, Eval RMSE:1.0322\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 540/567, Train_loss: 1.0376, Eval_loss:1.0175, Eval RMSE:1.0323\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "Fold:1/5 Epoch:3/3 Step: 560/567, Train_loss: 1.0374, Eval_loss:1.0175, Eval RMSE:1.0323\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:200\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 1, Epoch: 3 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold 1/5 Best Eval RMSE:0.5923193097114563, Best epoch:1, Best step:200\n",
      "\n",
      "#######################################################################\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2/5: Train fold: [4 3 2 5 1], Test fold:[3 2 4 5 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:2/5 Epoch:1/3 Step: 0/567, Train_loss: 1.2697, Eval_loss:1.2066, Eval RMSE:1.2116\n",
      "Eval RMSE improved from 100000.0000 to 1.2116\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 20/567, Train_loss: 1.1608, Eval_loss:0.9102, Eval RMSE:0.9142\n",
      "Eval RMSE improved from 1.2116 to 0.9142\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 40/567, Train_loss: 1.0180, Eval_loss:0.7519, Eval RMSE:0.7617\n",
      "Eval RMSE improved from 0.9142 to 0.7617\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 60/567, Train_loss: 0.9568, Eval_loss:0.7837, Eval RMSE:0.7901\n",
      "Eval RMSE did not improve from the 0.7617 from epoch:1 step:40\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 80/567, Train_loss: 0.9364, Eval_loss:1.2586, Eval RMSE:1.2607\n",
      "Eval RMSE did not improve from the 0.7617 from epoch:1 step:40\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 100/567, Train_loss: 0.9019, Eval_loss:0.8209, Eval RMSE:0.8181\n",
      "Eval RMSE did not improve from the 0.7617 from epoch:1 step:40\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 120/567, Train_loss: 0.8794, Eval_loss:0.6417, Eval RMSE:0.6437\n",
      "Eval RMSE improved from 0.7617 to 0.6437\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 140/567, Train_loss: 0.8594, Eval_loss:0.6487, Eval RMSE:0.6582\n",
      "Eval RMSE did not improve from the 0.6437 from epoch:1 step:120\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 160/567, Train_loss: 0.8397, Eval_loss:0.7262, Eval RMSE:0.7368\n",
      "Eval RMSE did not improve from the 0.6437 from epoch:1 step:120\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 180/567, Train_loss: 0.8389, Eval_loss:0.8380, Eval RMSE:0.8435\n",
      "Eval RMSE did not improve from the 0.6437 from epoch:1 step:120\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 200/567, Train_loss: 0.8302, Eval_loss:0.6358, Eval RMSE:0.6428\n",
      "Eval RMSE improved from 0.6437 to 0.6428\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 220/567, Train_loss: 0.8204, Eval_loss:0.6296, Eval RMSE:0.6375\n",
      "Eval RMSE improved from 0.6428 to 0.6375\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 240/567, Train_loss: 0.8113, Eval_loss:0.6381, Eval RMSE:0.6471\n",
      "Eval RMSE did not improve from the 0.6375 from epoch:1 step:220\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 260/567, Train_loss: 0.8010, Eval_loss:0.6113, Eval RMSE:0.6179\n",
      "Eval RMSE improved from 0.6375 to 0.6179\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 280/567, Train_loss: 0.7955, Eval_loss:0.6268, Eval RMSE:0.6325\n",
      "Eval RMSE did not improve from the 0.6179 from epoch:1 step:260\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 300/567, Train_loss: 0.7883, Eval_loss:0.7274, Eval RMSE:0.7216\n",
      "Eval RMSE did not improve from the 0.6179 from epoch:1 step:260\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 320/567, Train_loss: 0.7871, Eval_loss:0.6310, Eval RMSE:0.6352\n",
      "Eval RMSE did not improve from the 0.6179 from epoch:1 step:260\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 340/567, Train_loss: 0.7811, Eval_loss:0.5988, Eval RMSE:0.6024\n",
      "Eval RMSE improved from 0.6179 to 0.6024\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 360/567, Train_loss: 0.7714, Eval_loss:0.5854, Eval RMSE:0.5826\n",
      "Eval RMSE improved from 0.6024 to 0.5826\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 380/567, Train_loss: 0.7625, Eval_loss:0.5986, Eval RMSE:0.6012\n",
      "Eval RMSE did not improve from the 0.5826 from epoch:1 step:360\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 400/567, Train_loss: 0.7560, Eval_loss:0.5863, Eval RMSE:0.5815\n",
      "Eval RMSE improved from 0.5826 to 0.5815\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 420/567, Train_loss: 0.7499, Eval_loss:0.6107, Eval RMSE:0.6167\n",
      "Eval RMSE did not improve from the 0.5815 from epoch:1 step:400\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 440/567, Train_loss: 0.7423, Eval_loss:0.5642, Eval RMSE:0.5673\n",
      "Eval RMSE improved from 0.5815 to 0.5673\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 460/567, Train_loss: 0.7382, Eval_loss:0.6704, Eval RMSE:0.6788\n",
      "Eval RMSE did not improve from the 0.5673 from epoch:1 step:440\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 480/567, Train_loss: 0.7330, Eval_loss:0.5457, Eval RMSE:0.5434\n",
      "Eval RMSE improved from 0.5673 to 0.5434\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 500/567, Train_loss: 0.7270, Eval_loss:0.6097, Eval RMSE:0.6163\n",
      "Eval RMSE did not improve from the 0.5434 from epoch:1 step:480\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 520/567, Train_loss: 0.7215, Eval_loss:0.6283, Eval RMSE:0.6266\n",
      "Eval RMSE did not improve from the 0.5434 from epoch:1 step:480\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 540/567, Train_loss: 0.7157, Eval_loss:0.6301, Eval RMSE:0.6387\n",
      "Eval RMSE did not improve from the 0.5434 from epoch:1 step:480\n",
      "\n",
      "Fold:2/5 Epoch:1/3 Step: 560/567, Train_loss: 0.7108, Eval_loss:0.6238, Eval RMSE:0.6180\n",
      "Eval RMSE did not improve from the 0.5434 from epoch:1 step:480\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 2, Epoch: 1 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 0/567, Train_loss: 0.8607, Eval_loss:0.6029, Eval RMSE:0.6099\n",
      "Eval RMSE did not improve from the 0.5434 from epoch:1 step:480\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 20/567, Train_loss: 0.4484, Eval_loss:0.5477, Eval RMSE:0.5443\n",
      "Eval RMSE did not improve from the 0.5434 from epoch:1 step:480\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 40/567, Train_loss: 0.4419, Eval_loss:0.5562, Eval RMSE:0.5587\n",
      "Eval RMSE did not improve from the 0.5434 from epoch:1 step:480\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 60/567, Train_loss: 0.4675, Eval_loss:0.5378, Eval RMSE:0.5376\n",
      "Eval RMSE improved from 0.5434 to 0.5376\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 80/567, Train_loss: 0.4795, Eval_loss:0.5342, Eval RMSE:0.5327\n",
      "Eval RMSE improved from 0.5376 to 0.5327\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 100/567, Train_loss: 0.4725, Eval_loss:0.5759, Eval RMSE:0.5718\n",
      "Eval RMSE did not improve from the 0.5327 from epoch:2 step:80\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 120/567, Train_loss: 0.4591, Eval_loss:0.5499, Eval RMSE:0.5504\n",
      "Eval RMSE did not improve from the 0.5327 from epoch:2 step:80\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 140/567, Train_loss: 0.4559, Eval_loss:0.5730, Eval RMSE:0.5713\n",
      "Eval RMSE did not improve from the 0.5327 from epoch:2 step:80\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 160/567, Train_loss: 0.4507, Eval_loss:0.5421, Eval RMSE:0.5440\n",
      "Eval RMSE did not improve from the 0.5327 from epoch:2 step:80\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 180/567, Train_loss: 0.4493, Eval_loss:0.5202, Eval RMSE:0.5224\n",
      "Eval RMSE improved from 0.5327 to 0.5224\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 200/567, Train_loss: 0.4400, Eval_loss:0.5671, Eval RMSE:0.5675\n",
      "Eval RMSE did not improve from the 0.5224 from epoch:2 step:180\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 220/567, Train_loss: 0.4436, Eval_loss:0.5378, Eval RMSE:0.5377\n",
      "Eval RMSE did not improve from the 0.5224 from epoch:2 step:180\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 240/567, Train_loss: 0.4405, Eval_loss:0.5356, Eval RMSE:0.5354\n",
      "Eval RMSE did not improve from the 0.5224 from epoch:2 step:180\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 260/567, Train_loss: 0.4375, Eval_loss:0.5197, Eval RMSE:0.5210\n",
      "Eval RMSE improved from 0.5224 to 0.5210\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 280/567, Train_loss: 0.4356, Eval_loss:0.5293, Eval RMSE:0.5307\n",
      "Eval RMSE did not improve from the 0.5210 from epoch:2 step:260\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 300/567, Train_loss: 0.4319, Eval_loss:0.4937, Eval RMSE:0.4960\n",
      "Eval RMSE improved from 0.5210 to 0.4960\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 320/567, Train_loss: 0.4294, Eval_loss:0.5219, Eval RMSE:0.5193\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 340/567, Train_loss: 0.4281, Eval_loss:0.5583, Eval RMSE:0.5610\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 360/567, Train_loss: 0.4282, Eval_loss:0.5458, Eval RMSE:0.5475\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 380/567, Train_loss: 0.4282, Eval_loss:0.4952, Eval RMSE:0.4977\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 400/567, Train_loss: 0.4269, Eval_loss:0.5023, Eval RMSE:0.5062\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 420/567, Train_loss: 0.4260, Eval_loss:0.5283, Eval RMSE:0.5317\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 440/567, Train_loss: 0.4230, Eval_loss:0.4995, Eval RMSE:0.5016\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 460/567, Train_loss: 0.4223, Eval_loss:0.5245, Eval RMSE:0.5296\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 480/567, Train_loss: 0.4202, Eval_loss:0.5059, Eval RMSE:0.5067\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 500/567, Train_loss: 0.4228, Eval_loss:0.5075, Eval RMSE:0.5078\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 520/567, Train_loss: 0.4231, Eval_loss:0.5260, Eval RMSE:0.5241\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 540/567, Train_loss: 0.4227, Eval_loss:0.5406, Eval RMSE:0.5387\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:2/3 Step: 560/567, Train_loss: 0.4216, Eval_loss:0.5027, Eval RMSE:0.5040\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 2, Epoch: 2 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 0/567, Train_loss: 0.3500, Eval_loss:0.5304, Eval RMSE:0.5300\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 20/567, Train_loss: 0.2736, Eval_loss:0.4992, Eval RMSE:0.5007\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 40/567, Train_loss: 0.2776, Eval_loss:0.5068, Eval RMSE:0.5087\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 60/567, Train_loss: 0.2705, Eval_loss:0.4981, Eval RMSE:0.5005\n",
      "Eval RMSE did not improve from the 0.4960 from epoch:2 step:300\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 80/567, Train_loss: 0.2674, Eval_loss:0.4895, Eval RMSE:0.4909\n",
      "Eval RMSE improved from 0.4960 to 0.4909\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 100/567, Train_loss: 0.2659, Eval_loss:0.4900, Eval RMSE:0.4918\n",
      "Eval RMSE did not improve from the 0.4909 from epoch:3 step:80\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 120/567, Train_loss: 0.2577, Eval_loss:0.4865, Eval RMSE:0.4884\n",
      "Eval RMSE improved from 0.4909 to 0.4884\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 140/567, Train_loss: 0.2502, Eval_loss:0.4876, Eval RMSE:0.4898\n",
      "Eval RMSE did not improve from the 0.4884 from epoch:3 step:120\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 160/567, Train_loss: 0.2448, Eval_loss:0.4853, Eval RMSE:0.4874\n",
      "Eval RMSE improved from 0.4884 to 0.4874\n",
      "Saving the model ./fold_2_Roberta_large_model.bin\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 180/567, Train_loss: 0.2417, Eval_loss:0.4891, Eval RMSE:0.4913\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 200/567, Train_loss: 0.2386, Eval_loss:0.5002, Eval RMSE:0.5032\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 220/567, Train_loss: 0.2375, Eval_loss:0.5057, Eval RMSE:0.5072\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 240/567, Train_loss: 0.2367, Eval_loss:0.4876, Eval RMSE:0.4909\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 260/567, Train_loss: 0.2368, Eval_loss:0.4938, Eval RMSE:0.4965\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 280/567, Train_loss: 0.2393, Eval_loss:0.4916, Eval RMSE:0.4955\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 300/567, Train_loss: 0.2394, Eval_loss:0.4894, Eval RMSE:0.4931\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 320/567, Train_loss: 0.2406, Eval_loss:0.4897, Eval RMSE:0.4925\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 340/567, Train_loss: 0.2395, Eval_loss:0.4938, Eval RMSE:0.4960\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 360/567, Train_loss: 0.2379, Eval_loss:0.4987, Eval RMSE:0.5036\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 380/567, Train_loss: 0.2397, Eval_loss:0.4895, Eval RMSE:0.4927\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 400/567, Train_loss: 0.2410, Eval_loss:0.4893, Eval RMSE:0.4920\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 420/567, Train_loss: 0.2391, Eval_loss:0.4970, Eval RMSE:0.4985\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 440/567, Train_loss: 0.2384, Eval_loss:0.4873, Eval RMSE:0.4907\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 460/567, Train_loss: 0.2369, Eval_loss:0.4876, Eval RMSE:0.4902\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 480/567, Train_loss: 0.2355, Eval_loss:0.4889, Eval RMSE:0.4911\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 500/567, Train_loss: 0.2354, Eval_loss:0.4932, Eval RMSE:0.4947\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 520/567, Train_loss: 0.2350, Eval_loss:0.4910, Eval RMSE:0.4927\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 540/567, Train_loss: 0.2341, Eval_loss:0.4893, Eval RMSE:0.4912\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "Fold:2/5 Epoch:3/3 Step: 560/567, Train_loss: 0.2358, Eval_loss:0.4877, Eval RMSE:0.4898\n",
      "Eval RMSE did not improve from the 0.4874 from epoch:3 step:160\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 2, Epoch: 3 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold 2/5 Best Eval RMSE:0.48741987347602844, Best epoch:3, Best step:160\n",
      "\n",
      "#######################################################################\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3/5: Train fold: [4 3 2 5 1], Test fold:[5 4 2 1 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:3/5 Epoch:1/3 Step: 0/567, Train_loss: 1.2877, Eval_loss:1.0146, Eval RMSE:1.0274\n",
      "Eval RMSE improved from 100000.0000 to 1.0274\n",
      "Saving the model ./fold_3_Roberta_large_model.bin\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 20/567, Train_loss: 0.9611, Eval_loss:0.8671, Eval RMSE:0.8718\n",
      "Eval RMSE improved from 1.0274 to 0.8718\n",
      "Saving the model ./fold_3_Roberta_large_model.bin\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 40/567, Train_loss: 0.8461, Eval_loss:0.9577, Eval RMSE:0.9614\n",
      "Eval RMSE did not improve from the 0.8718 from epoch:1 step:20\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 60/567, Train_loss: 0.8254, Eval_loss:0.7300, Eval RMSE:0.7403\n",
      "Eval RMSE improved from 0.8718 to 0.7403\n",
      "Saving the model ./fold_3_Roberta_large_model.bin\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 80/567, Train_loss: 0.8450, Eval_loss:0.7842, Eval RMSE:0.7952\n",
      "Eval RMSE did not improve from the 0.7403 from epoch:1 step:60\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 100/567, Train_loss: 0.8381, Eval_loss:0.7064, Eval RMSE:0.7079\n",
      "Eval RMSE improved from 0.7403 to 0.7079\n",
      "Saving the model ./fold_3_Roberta_large_model.bin\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 120/567, Train_loss: 0.8311, Eval_loss:0.7495, Eval RMSE:0.7593\n",
      "Eval RMSE did not improve from the 0.7079 from epoch:1 step:100\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 140/567, Train_loss: 0.8149, Eval_loss:0.6182, Eval RMSE:0.6216\n",
      "Eval RMSE improved from 0.7079 to 0.6216\n",
      "Saving the model ./fold_3_Roberta_large_model.bin\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 160/567, Train_loss: 0.8098, Eval_loss:0.8541, Eval RMSE:0.8591\n",
      "Eval RMSE did not improve from the 0.6216 from epoch:1 step:140\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 180/567, Train_loss: 0.8001, Eval_loss:0.6321, Eval RMSE:0.6342\n",
      "Eval RMSE did not improve from the 0.6216 from epoch:1 step:140\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 200/567, Train_loss: 0.7930, Eval_loss:0.6803, Eval RMSE:0.6846\n",
      "Eval RMSE did not improve from the 0.6216 from epoch:1 step:140\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 220/567, Train_loss: 0.7828, Eval_loss:0.6184, Eval RMSE:0.6242\n",
      "Eval RMSE did not improve from the 0.6216 from epoch:1 step:140\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 240/567, Train_loss: 0.7881, Eval_loss:0.8308, Eval RMSE:0.8396\n",
      "Eval RMSE did not improve from the 0.6216 from epoch:1 step:140\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 260/567, Train_loss: 0.7833, Eval_loss:0.6301, Eval RMSE:0.6398\n",
      "Eval RMSE did not improve from the 0.6216 from epoch:1 step:140\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 280/567, Train_loss: 0.7798, Eval_loss:0.6976, Eval RMSE:0.7061\n",
      "Eval RMSE did not improve from the 0.6216 from epoch:1 step:140\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 300/567, Train_loss: 0.7712, Eval_loss:0.5455, Eval RMSE:0.5497\n",
      "Eval RMSE improved from 0.6216 to 0.5497\n",
      "Saving the model ./fold_3_Roberta_large_model.bin\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 320/567, Train_loss: 0.7668, Eval_loss:0.5833, Eval RMSE:0.5894\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 340/567, Train_loss: 0.7606, Eval_loss:0.5753, Eval RMSE:0.5767\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 360/567, Train_loss: 0.7512, Eval_loss:0.6195, Eval RMSE:0.6179\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 380/567, Train_loss: 0.7485, Eval_loss:0.5812, Eval RMSE:0.5863\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 400/567, Train_loss: 0.7399, Eval_loss:0.6283, Eval RMSE:0.6311\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 420/567, Train_loss: 0.7310, Eval_loss:0.5474, Eval RMSE:0.5518\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 440/567, Train_loss: 0.7589, Eval_loss:1.1374, Eval RMSE:1.1552\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 460/567, Train_loss: 0.8032, Eval_loss:1.0267, Eval RMSE:1.0382\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 480/567, Train_loss: 0.8154, Eval_loss:1.0218, Eval RMSE:1.0350\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 500/567, Train_loss: 0.8207, Eval_loss:1.0518, Eval RMSE:1.0611\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 520/567, Train_loss: 0.8298, Eval_loss:1.0422, Eval RMSE:1.0576\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 540/567, Train_loss: 0.8389, Eval_loss:1.0597, Eval RMSE:1.0686\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:1/3 Step: 560/567, Train_loss: 0.8475, Eval_loss:1.0311, Eval RMSE:1.0421\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 3, Epoch: 1 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 0/567, Train_loss: 0.8225, Eval_loss:1.0228, Eval RMSE:1.0351\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 20/567, Train_loss: 1.0058, Eval_loss:1.0299, Eval RMSE:1.0411\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 40/567, Train_loss: 1.0110, Eval_loss:1.0287, Eval RMSE:1.0400\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 60/567, Train_loss: 1.0111, Eval_loss:1.0779, Eval RMSE:1.0859\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 80/567, Train_loss: 0.9750, Eval_loss:1.0738, Eval RMSE:1.0820\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 100/567, Train_loss: 0.9834, Eval_loss:1.0221, Eval RMSE:1.0356\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 120/567, Train_loss: 0.9950, Eval_loss:1.0390, Eval RMSE:1.0543\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 140/567, Train_loss: 0.9822, Eval_loss:1.0295, Eval RMSE:1.0440\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 160/567, Train_loss: 1.0056, Eval_loss:1.0227, Eval RMSE:1.0363\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 180/567, Train_loss: 1.0103, Eval_loss:1.0285, Eval RMSE:1.0398\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 200/567, Train_loss: 1.0173, Eval_loss:1.0720, Eval RMSE:1.0884\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 220/567, Train_loss: 1.0101, Eval_loss:1.0290, Eval RMSE:1.0435\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 240/567, Train_loss: 1.0153, Eval_loss:1.0360, Eval RMSE:1.0466\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 260/567, Train_loss: 1.0178, Eval_loss:1.0269, Eval RMSE:1.0384\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 280/567, Train_loss: 1.0171, Eval_loss:1.0218, Eval RMSE:1.0351\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 300/567, Train_loss: 1.0160, Eval_loss:1.0239, Eval RMSE:1.0359\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 320/567, Train_loss: 1.0138, Eval_loss:1.0277, Eval RMSE:1.0392\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 340/567, Train_loss: 1.0152, Eval_loss:1.0238, Eval RMSE:1.0376\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 360/567, Train_loss: 1.0165, Eval_loss:1.0316, Eval RMSE:1.0426\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 380/567, Train_loss: 1.0220, Eval_loss:1.0225, Eval RMSE:1.0349\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 400/567, Train_loss: 1.0245, Eval_loss:1.0216, Eval RMSE:1.0345\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 420/567, Train_loss: 1.0222, Eval_loss:1.0225, Eval RMSE:1.0360\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 440/567, Train_loss: 1.0245, Eval_loss:1.0229, Eval RMSE:1.0366\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 460/567, Train_loss: 1.0260, Eval_loss:1.0634, Eval RMSE:1.0797\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 480/567, Train_loss: 1.0259, Eval_loss:1.0417, Eval RMSE:1.0571\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 500/567, Train_loss: 1.0329, Eval_loss:1.0226, Eval RMSE:1.0361\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 520/567, Train_loss: 1.0341, Eval_loss:1.0234, Eval RMSE:1.0355\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 540/567, Train_loss: 1.0327, Eval_loss:1.0223, Eval RMSE:1.0347\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:2/3 Step: 560/567, Train_loss: 1.0328, Eval_loss:1.0261, Eval RMSE:1.0378\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 3, Epoch: 2 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 0/567, Train_loss: 1.1137, Eval_loss:1.0237, Eval RMSE:1.0358\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 20/567, Train_loss: 1.0103, Eval_loss:1.0216, Eval RMSE:1.0346\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 40/567, Train_loss: 1.0385, Eval_loss:1.0216, Eval RMSE:1.0345\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 60/567, Train_loss: 0.9889, Eval_loss:1.0217, Eval RMSE:1.0349\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 80/567, Train_loss: 1.0244, Eval_loss:1.0253, Eval RMSE:1.0393\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 100/567, Train_loss: 1.0305, Eval_loss:1.0219, Eval RMSE:1.0345\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 120/567, Train_loss: 1.0195, Eval_loss:1.0317, Eval RMSE:1.0427\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 140/567, Train_loss: 1.0217, Eval_loss:1.0216, Eval RMSE:1.0345\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 160/567, Train_loss: 1.0307, Eval_loss:1.0216, Eval RMSE:1.0346\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 180/567, Train_loss: 1.0348, Eval_loss:1.0218, Eval RMSE:1.0345\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 200/567, Train_loss: 1.0376, Eval_loss:1.0216, Eval RMSE:1.0346\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 220/567, Train_loss: 1.0358, Eval_loss:1.0238, Eval RMSE:1.0377\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 240/567, Train_loss: 1.0319, Eval_loss:1.0216, Eval RMSE:1.0348\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 260/567, Train_loss: 1.0329, Eval_loss:1.0247, Eval RMSE:1.0366\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 280/567, Train_loss: 1.0351, Eval_loss:1.0217, Eval RMSE:1.0345\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 300/567, Train_loss: 1.0343, Eval_loss:1.0233, Eval RMSE:1.0370\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 320/567, Train_loss: 1.0313, Eval_loss:1.0228, Eval RMSE:1.0351\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 340/567, Train_loss: 1.0352, Eval_loss:1.0238, Eval RMSE:1.0359\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 360/567, Train_loss: 1.0379, Eval_loss:1.0216, Eval RMSE:1.0347\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 380/567, Train_loss: 1.0349, Eval_loss:1.0218, Eval RMSE:1.0351\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 400/567, Train_loss: 1.0366, Eval_loss:1.0216, Eval RMSE:1.0345\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 420/567, Train_loss: 1.0365, Eval_loss:1.0220, Eval RMSE:1.0346\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 440/567, Train_loss: 1.0385, Eval_loss:1.0218, Eval RMSE:1.0345\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 460/567, Train_loss: 1.0364, Eval_loss:1.0227, Eval RMSE:1.0350\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 480/567, Train_loss: 1.0339, Eval_loss:1.0236, Eval RMSE:1.0357\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 500/567, Train_loss: 1.0353, Eval_loss:1.0234, Eval RMSE:1.0355\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 520/567, Train_loss: 1.0351, Eval_loss:1.0226, Eval RMSE:1.0349\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 540/567, Train_loss: 1.0324, Eval_loss:1.0225, Eval RMSE:1.0349\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "Fold:3/5 Epoch:3/3 Step: 560/567, Train_loss: 1.0380, Eval_loss:1.0224, Eval RMSE:1.0349\n",
      "Eval RMSE did not improve from the 0.5497 from epoch:1 step:300\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 3, Epoch: 3 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold 3/5 Best Eval RMSE:0.5497123003005981, Best epoch:1, Best step:300\n",
      "\n",
      "#######################################################################\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4/5: Train fold: [4 3 2 5 1], Test fold:[2 5 4 3 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:4/5 Epoch:1/3 Step: 0/567, Train_loss: 0.7176, Eval_loss:1.0510, Eval RMSE:1.0498\n",
      "Eval RMSE improved from 100000.0000 to 1.0498\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 20/567, Train_loss: 1.0426, Eval_loss:0.9794, Eval RMSE:0.9776\n",
      "Eval RMSE improved from 1.0498 to 0.9776\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 40/567, Train_loss: 0.9771, Eval_loss:0.8713, Eval RMSE:0.8715\n",
      "Eval RMSE improved from 0.9776 to 0.8715\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 60/567, Train_loss: 0.9322, Eval_loss:0.8804, Eval RMSE:0.8896\n",
      "Eval RMSE did not improve from the 0.8715 from epoch:1 step:40\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 80/567, Train_loss: 0.9095, Eval_loss:0.7822, Eval RMSE:0.7703\n",
      "Eval RMSE improved from 0.8715 to 0.7703\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 100/567, Train_loss: 0.8896, Eval_loss:0.7548, Eval RMSE:0.7436\n",
      "Eval RMSE improved from 0.7703 to 0.7436\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 120/567, Train_loss: 0.8560, Eval_loss:0.6328, Eval RMSE:0.6246\n",
      "Eval RMSE improved from 0.7436 to 0.6246\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 140/567, Train_loss: 0.8311, Eval_loss:0.6786, Eval RMSE:0.6766\n",
      "Eval RMSE did not improve from the 0.6246 from epoch:1 step:120\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 160/567, Train_loss: 0.8165, Eval_loss:0.6657, Eval RMSE:0.6616\n",
      "Eval RMSE did not improve from the 0.6246 from epoch:1 step:120\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 180/567, Train_loss: 0.8003, Eval_loss:0.7043, Eval RMSE:0.7030\n",
      "Eval RMSE did not improve from the 0.6246 from epoch:1 step:120\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 200/567, Train_loss: 0.7874, Eval_loss:0.8908, Eval RMSE:0.8929\n",
      "Eval RMSE did not improve from the 0.6246 from epoch:1 step:120\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 220/567, Train_loss: 0.7813, Eval_loss:0.6307, Eval RMSE:0.6248\n",
      "Eval RMSE did not improve from the 0.6246 from epoch:1 step:120\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 240/567, Train_loss: 0.7755, Eval_loss:0.9510, Eval RMSE:0.9438\n",
      "Eval RMSE did not improve from the 0.6246 from epoch:1 step:120\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 260/567, Train_loss: 0.7781, Eval_loss:0.8214, Eval RMSE:0.8152\n",
      "Eval RMSE did not improve from the 0.6246 from epoch:1 step:120\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 280/567, Train_loss: 0.7693, Eval_loss:0.6358, Eval RMSE:0.6168\n",
      "Eval RMSE improved from 0.6246 to 0.6168\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 300/567, Train_loss: 0.7599, Eval_loss:0.6829, Eval RMSE:0.6827\n",
      "Eval RMSE did not improve from the 0.6168 from epoch:1 step:280\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 320/567, Train_loss: 0.7538, Eval_loss:0.6616, Eval RMSE:0.6593\n",
      "Eval RMSE did not improve from the 0.6168 from epoch:1 step:280\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 340/567, Train_loss: 0.7469, Eval_loss:0.6944, Eval RMSE:0.6871\n",
      "Eval RMSE did not improve from the 0.6168 from epoch:1 step:280\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 360/567, Train_loss: 0.7425, Eval_loss:0.6242, Eval RMSE:0.6171\n",
      "Eval RMSE did not improve from the 0.6168 from epoch:1 step:280\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 380/567, Train_loss: 0.7366, Eval_loss:0.7080, Eval RMSE:0.6977\n",
      "Eval RMSE did not improve from the 0.6168 from epoch:1 step:280\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 400/567, Train_loss: 0.7270, Eval_loss:0.6074, Eval RMSE:0.5966\n",
      "Eval RMSE improved from 0.6168 to 0.5966\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 420/567, Train_loss: 0.7208, Eval_loss:0.7505, Eval RMSE:0.7384\n",
      "Eval RMSE did not improve from the 0.5966 from epoch:1 step:400\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 440/567, Train_loss: 0.7169, Eval_loss:0.6594, Eval RMSE:0.6596\n",
      "Eval RMSE did not improve from the 0.5966 from epoch:1 step:400\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 460/567, Train_loss: 0.7161, Eval_loss:0.6016, Eval RMSE:0.5923\n",
      "Eval RMSE improved from 0.5966 to 0.5923\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 480/567, Train_loss: 0.7096, Eval_loss:0.7411, Eval RMSE:0.7276\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:460\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 500/567, Train_loss: 0.7079, Eval_loss:0.5920, Eval RMSE:0.5929\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:460\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 520/567, Train_loss: 0.7033, Eval_loss:0.8902, Eval RMSE:0.8719\n",
      "Eval RMSE did not improve from the 0.5923 from epoch:1 step:460\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 540/567, Train_loss: 0.7013, Eval_loss:0.5987, Eval RMSE:0.5853\n",
      "Eval RMSE improved from 0.5923 to 0.5853\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:1/3 Step: 560/567, Train_loss: 0.6967, Eval_loss:0.5859, Eval RMSE:0.5779\n",
      "Eval RMSE improved from 0.5853 to 0.5779\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 4, Epoch: 1 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 0/567, Train_loss: 0.4809, Eval_loss:0.7118, Eval RMSE:0.7149\n",
      "Eval RMSE did not improve from the 0.5779 from epoch:1 step:560\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 20/567, Train_loss: 0.4895, Eval_loss:0.5970, Eval RMSE:0.5881\n",
      "Eval RMSE did not improve from the 0.5779 from epoch:1 step:560\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 40/567, Train_loss: 0.4910, Eval_loss:0.6483, Eval RMSE:0.6459\n",
      "Eval RMSE did not improve from the 0.5779 from epoch:1 step:560\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 60/567, Train_loss: 0.4779, Eval_loss:0.6216, Eval RMSE:0.6173\n",
      "Eval RMSE did not improve from the 0.5779 from epoch:1 step:560\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 80/567, Train_loss: 0.4596, Eval_loss:0.5554, Eval RMSE:0.5437\n",
      "Eval RMSE improved from 0.5779 to 0.5437\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 100/567, Train_loss: 0.4507, Eval_loss:0.5510, Eval RMSE:0.5398\n",
      "Eval RMSE improved from 0.5437 to 0.5398\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 120/567, Train_loss: 0.4489, Eval_loss:0.6118, Eval RMSE:0.6100\n",
      "Eval RMSE did not improve from the 0.5398 from epoch:2 step:100\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 140/567, Train_loss: 0.4452, Eval_loss:0.5677, Eval RMSE:0.5592\n",
      "Eval RMSE did not improve from the 0.5398 from epoch:2 step:100\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 160/567, Train_loss: 0.4406, Eval_loss:0.5675, Eval RMSE:0.5504\n",
      "Eval RMSE did not improve from the 0.5398 from epoch:2 step:100\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 180/567, Train_loss: 0.4312, Eval_loss:0.5514, Eval RMSE:0.5361\n",
      "Eval RMSE improved from 0.5398 to 0.5361\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 200/567, Train_loss: 0.4304, Eval_loss:0.5506, Eval RMSE:0.5411\n",
      "Eval RMSE did not improve from the 0.5361 from epoch:2 step:180\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 220/567, Train_loss: 0.4297, Eval_loss:0.5940, Eval RMSE:0.5891\n",
      "Eval RMSE did not improve from the 0.5361 from epoch:2 step:180\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 240/567, Train_loss: 0.4295, Eval_loss:0.5401, Eval RMSE:0.5287\n",
      "Eval RMSE improved from 0.5361 to 0.5287\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 260/567, Train_loss: 0.4237, Eval_loss:0.5463, Eval RMSE:0.5350\n",
      "Eval RMSE did not improve from the 0.5287 from epoch:2 step:240\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 280/567, Train_loss: 0.4224, Eval_loss:0.5477, Eval RMSE:0.5426\n",
      "Eval RMSE did not improve from the 0.5287 from epoch:2 step:240\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 300/567, Train_loss: 0.4192, Eval_loss:0.5581, Eval RMSE:0.5466\n",
      "Eval RMSE did not improve from the 0.5287 from epoch:2 step:240\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 320/567, Train_loss: 0.4177, Eval_loss:0.5294, Eval RMSE:0.5179\n",
      "Eval RMSE improved from 0.5287 to 0.5179\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 340/567, Train_loss: 0.4188, Eval_loss:0.5536, Eval RMSE:0.5516\n",
      "Eval RMSE did not improve from the 0.5179 from epoch:2 step:320\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 360/567, Train_loss: 0.4153, Eval_loss:0.5292, Eval RMSE:0.5240\n",
      "Eval RMSE did not improve from the 0.5179 from epoch:2 step:320\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 380/567, Train_loss: 0.4123, Eval_loss:0.5316, Eval RMSE:0.5247\n",
      "Eval RMSE did not improve from the 0.5179 from epoch:2 step:320\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 400/567, Train_loss: 0.4103, Eval_loss:0.5363, Eval RMSE:0.5312\n",
      "Eval RMSE did not improve from the 0.5179 from epoch:2 step:320\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 420/567, Train_loss: 0.4094, Eval_loss:0.5190, Eval RMSE:0.5118\n",
      "Eval RMSE improved from 0.5179 to 0.5118\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 440/567, Train_loss: 0.4069, Eval_loss:0.5154, Eval RMSE:0.5044\n",
      "Eval RMSE improved from 0.5118 to 0.5044\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 460/567, Train_loss: 0.4066, Eval_loss:0.5129, Eval RMSE:0.5024\n",
      "Eval RMSE improved from 0.5044 to 0.5024\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 480/567, Train_loss: 0.4075, Eval_loss:0.6154, Eval RMSE:0.6031\n",
      "Eval RMSE did not improve from the 0.5024 from epoch:2 step:460\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 500/567, Train_loss: 0.4072, Eval_loss:0.5685, Eval RMSE:0.5538\n",
      "Eval RMSE did not improve from the 0.5024 from epoch:2 step:460\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 520/567, Train_loss: 0.4092, Eval_loss:0.6315, Eval RMSE:0.6202\n",
      "Eval RMSE did not improve from the 0.5024 from epoch:2 step:460\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 540/567, Train_loss: 0.4099, Eval_loss:0.5534, Eval RMSE:0.5409\n",
      "Eval RMSE did not improve from the 0.5024 from epoch:2 step:460\n",
      "\n",
      "Fold:4/5 Epoch:2/3 Step: 560/567, Train_loss: 0.4125, Eval_loss:0.5969, Eval RMSE:0.5842\n",
      "Eval RMSE did not improve from the 0.5024 from epoch:2 step:460\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 4, Epoch: 2 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 0/567, Train_loss: 0.3843, Eval_loss:0.5275, Eval RMSE:0.5205\n",
      "Eval RMSE did not improve from the 0.5024 from epoch:2 step:460\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 20/567, Train_loss: 0.2461, Eval_loss:0.5299, Eval RMSE:0.5212\n",
      "Eval RMSE did not improve from the 0.5024 from epoch:2 step:460\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 40/567, Train_loss: 0.2458, Eval_loss:0.5178, Eval RMSE:0.5059\n",
      "Eval RMSE did not improve from the 0.5024 from epoch:2 step:460\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 60/567, Train_loss: 0.2455, Eval_loss:0.5288, Eval RMSE:0.5226\n",
      "Eval RMSE did not improve from the 0.5024 from epoch:2 step:460\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 80/567, Train_loss: 0.2431, Eval_loss:0.5108, Eval RMSE:0.5019\n",
      "Eval RMSE improved from 0.5024 to 0.5019\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 100/567, Train_loss: 0.2390, Eval_loss:0.5433, Eval RMSE:0.5282\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 120/567, Train_loss: 0.2324, Eval_loss:0.5192, Eval RMSE:0.5111\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 140/567, Train_loss: 0.2278, Eval_loss:0.5229, Eval RMSE:0.5165\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 160/567, Train_loss: 0.2244, Eval_loss:0.5245, Eval RMSE:0.5159\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 180/567, Train_loss: 0.2246, Eval_loss:0.5114, Eval RMSE:0.5023\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 200/567, Train_loss: 0.2251, Eval_loss:0.5138, Eval RMSE:0.5067\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 220/567, Train_loss: 0.2218, Eval_loss:0.5160, Eval RMSE:0.5092\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 240/567, Train_loss: 0.2190, Eval_loss:0.5138, Eval RMSE:0.5058\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 260/567, Train_loss: 0.2181, Eval_loss:0.5191, Eval RMSE:0.5111\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 280/567, Train_loss: 0.2176, Eval_loss:0.5144, Eval RMSE:0.5062\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 300/567, Train_loss: 0.2179, Eval_loss:0.5135, Eval RMSE:0.5074\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 320/567, Train_loss: 0.2165, Eval_loss:0.5184, Eval RMSE:0.5105\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 340/567, Train_loss: 0.2158, Eval_loss:0.5114, Eval RMSE:0.5032\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 360/567, Train_loss: 0.2159, Eval_loss:0.5127, Eval RMSE:0.5036\n",
      "Eval RMSE did not improve from the 0.5019 from epoch:3 step:80\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 380/567, Train_loss: 0.2159, Eval_loss:0.5091, Eval RMSE:0.5008\n",
      "Eval RMSE improved from 0.5019 to 0.5008\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 400/567, Train_loss: 0.2151, Eval_loss:0.5064, Eval RMSE:0.4985\n",
      "Eval RMSE improved from 0.5008 to 0.4985\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 420/567, Train_loss: 0.2154, Eval_loss:0.5119, Eval RMSE:0.5039\n",
      "Eval RMSE did not improve from the 0.4985 from epoch:3 step:400\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 440/567, Train_loss: 0.2147, Eval_loss:0.5106, Eval RMSE:0.5021\n",
      "Eval RMSE did not improve from the 0.4985 from epoch:3 step:400\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 460/567, Train_loss: 0.2132, Eval_loss:0.5070, Eval RMSE:0.4993\n",
      "Eval RMSE did not improve from the 0.4985 from epoch:3 step:400\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 480/567, Train_loss: 0.2132, Eval_loss:0.5056, Eval RMSE:0.4974\n",
      "Eval RMSE improved from 0.4985 to 0.4974\n",
      "Saving the model ./fold_4_Roberta_large_model.bin\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 500/567, Train_loss: 0.2132, Eval_loss:0.5063, Eval RMSE:0.4984\n",
      "Eval RMSE did not improve from the 0.4974 from epoch:3 step:480\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 520/567, Train_loss: 0.2127, Eval_loss:0.5072, Eval RMSE:0.4989\n",
      "Eval RMSE did not improve from the 0.4974 from epoch:3 step:480\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 540/567, Train_loss: 0.2118, Eval_loss:0.5078, Eval RMSE:0.4991\n",
      "Eval RMSE did not improve from the 0.4974 from epoch:3 step:480\n",
      "\n",
      "Fold:4/5 Epoch:3/3 Step: 560/567, Train_loss: 0.2106, Eval_loss:0.5082, Eval RMSE:0.4994\n",
      "Eval RMSE did not improve from the 0.4974 from epoch:3 step:480\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 4, Epoch: 3 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold 4/5 Best Eval RMSE:0.497443825006485, Best epoch:3, Best step:480\n",
      "\n",
      "#######################################################################\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5/5: Train fold: [4 3 2 5 1], Test fold:[3 2 4 1 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/huggingface-roberta/roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:5/5 Epoch:1/3 Step: 0/567, Train_loss: 1.3555, Eval_loss:1.2057, Eval RMSE:1.1944\n",
      "Eval RMSE improved from 100000.0000 to 1.1944\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 20/567, Train_loss: 1.0528, Eval_loss:0.8392, Eval RMSE:0.8359\n",
      "Eval RMSE improved from 1.1944 to 0.8359\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 40/567, Train_loss: 0.9655, Eval_loss:0.8425, Eval RMSE:0.8378\n",
      "Eval RMSE did not improve from the 0.8359 from epoch:1 step:20\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 60/567, Train_loss: 0.9335, Eval_loss:0.7648, Eval RMSE:0.7648\n",
      "Eval RMSE improved from 0.8359 to 0.7648\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 80/567, Train_loss: 0.8872, Eval_loss:0.7462, Eval RMSE:0.7397\n",
      "Eval RMSE improved from 0.7648 to 0.7397\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 100/567, Train_loss: 0.8510, Eval_loss:0.8441, Eval RMSE:0.8434\n",
      "Eval RMSE did not improve from the 0.7397 from epoch:1 step:80\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 120/567, Train_loss: 0.8399, Eval_loss:0.7103, Eval RMSE:0.7067\n",
      "Eval RMSE improved from 0.7397 to 0.7067\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 140/567, Train_loss: 0.8212, Eval_loss:0.7020, Eval RMSE:0.6974\n",
      "Eval RMSE improved from 0.7067 to 0.6974\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 160/567, Train_loss: 0.8099, Eval_loss:0.7765, Eval RMSE:0.7738\n",
      "Eval RMSE did not improve from the 0.6974 from epoch:1 step:140\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 180/567, Train_loss: 0.8034, Eval_loss:0.6579, Eval RMSE:0.6549\n",
      "Eval RMSE improved from 0.6974 to 0.6549\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 200/567, Train_loss: 0.7845, Eval_loss:0.6624, Eval RMSE:0.6628\n",
      "Eval RMSE did not improve from the 0.6549 from epoch:1 step:180\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 220/567, Train_loss: 0.7809, Eval_loss:0.8077, Eval RMSE:0.8094\n",
      "Eval RMSE did not improve from the 0.6549 from epoch:1 step:180\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 240/567, Train_loss: 0.7737, Eval_loss:0.7408, Eval RMSE:0.7437\n",
      "Eval RMSE did not improve from the 0.6549 from epoch:1 step:180\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 260/567, Train_loss: 0.7657, Eval_loss:0.7321, Eval RMSE:0.7277\n",
      "Eval RMSE did not improve from the 0.6549 from epoch:1 step:180\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 280/567, Train_loss: 0.7676, Eval_loss:0.6971, Eval RMSE:0.6954\n",
      "Eval RMSE did not improve from the 0.6549 from epoch:1 step:180\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 300/567, Train_loss: 0.7524, Eval_loss:0.6491, Eval RMSE:0.6482\n",
      "Eval RMSE improved from 0.6549 to 0.6482\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 320/567, Train_loss: 0.7461, Eval_loss:0.6687, Eval RMSE:0.6643\n",
      "Eval RMSE did not improve from the 0.6482 from epoch:1 step:300\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 340/567, Train_loss: 0.7471, Eval_loss:0.6910, Eval RMSE:0.6871\n",
      "Eval RMSE did not improve from the 0.6482 from epoch:1 step:300\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 360/567, Train_loss: 0.7420, Eval_loss:0.6728, Eval RMSE:0.6693\n",
      "Eval RMSE did not improve from the 0.6482 from epoch:1 step:300\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 380/567, Train_loss: 0.7381, Eval_loss:0.6511, Eval RMSE:0.6485\n",
      "Eval RMSE did not improve from the 0.6482 from epoch:1 step:300\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 400/567, Train_loss: 0.7334, Eval_loss:0.6178, Eval RMSE:0.6127\n",
      "Eval RMSE improved from 0.6482 to 0.6127\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 420/567, Train_loss: 0.7261, Eval_loss:0.6078, Eval RMSE:0.6026\n",
      "Eval RMSE improved from 0.6127 to 0.6026\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 440/567, Train_loss: 0.7191, Eval_loss:0.5874, Eval RMSE:0.5814\n",
      "Eval RMSE improved from 0.6026 to 0.5814\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 460/567, Train_loss: 0.7192, Eval_loss:0.6220, Eval RMSE:0.6193\n",
      "Eval RMSE did not improve from the 0.5814 from epoch:1 step:440\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 480/567, Train_loss: 0.7122, Eval_loss:0.5853, Eval RMSE:0.5828\n",
      "Eval RMSE did not improve from the 0.5814 from epoch:1 step:440\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 500/567, Train_loss: 0.7080, Eval_loss:0.6032, Eval RMSE:0.6008\n",
      "Eval RMSE did not improve from the 0.5814 from epoch:1 step:440\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 520/567, Train_loss: 0.7043, Eval_loss:0.7163, Eval RMSE:0.7108\n",
      "Eval RMSE did not improve from the 0.5814 from epoch:1 step:440\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 540/567, Train_loss: 0.7021, Eval_loss:0.5850, Eval RMSE:0.5797\n",
      "Eval RMSE improved from 0.5814 to 0.5797\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:1/3 Step: 560/567, Train_loss: 0.6982, Eval_loss:0.5854, Eval RMSE:0.5819\n",
      "Eval RMSE did not improve from the 0.5797 from epoch:1 step:540\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 5, Epoch: 1 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 0/567, Train_loss: 0.3551, Eval_loss:0.6082, Eval RMSE:0.6028\n",
      "Eval RMSE did not improve from the 0.5797 from epoch:1 step:540\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 20/567, Train_loss: 0.3605, Eval_loss:0.5552, Eval RMSE:0.5458\n",
      "Eval RMSE improved from 0.5797 to 0.5458\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 40/567, Train_loss: 0.3718, Eval_loss:0.5574, Eval RMSE:0.5489\n",
      "Eval RMSE did not improve from the 0.5458 from epoch:2 step:20\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 60/567, Train_loss: 0.3699, Eval_loss:0.5623, Eval RMSE:0.5551\n",
      "Eval RMSE did not improve from the 0.5458 from epoch:2 step:20\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 80/567, Train_loss: 0.3722, Eval_loss:0.6463, Eval RMSE:0.6439\n",
      "Eval RMSE did not improve from the 0.5458 from epoch:2 step:20\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 100/567, Train_loss: 0.4525, Eval_loss:0.8506, Eval RMSE:0.8436\n",
      "Eval RMSE did not improve from the 0.5458 from epoch:2 step:20\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 120/567, Train_loss: 0.4827, Eval_loss:0.5777, Eval RMSE:0.5730\n",
      "Eval RMSE did not improve from the 0.5458 from epoch:2 step:20\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 140/567, Train_loss: 0.4792, Eval_loss:0.7087, Eval RMSE:0.6990\n",
      "Eval RMSE did not improve from the 0.5458 from epoch:2 step:20\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 160/567, Train_loss: 0.4788, Eval_loss:0.6216, Eval RMSE:0.6153\n",
      "Eval RMSE did not improve from the 0.5458 from epoch:2 step:20\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 180/567, Train_loss: 0.4830, Eval_loss:0.6286, Eval RMSE:0.6245\n",
      "Eval RMSE did not improve from the 0.5458 from epoch:2 step:20\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 200/567, Train_loss: 0.4800, Eval_loss:0.5590, Eval RMSE:0.5530\n",
      "Eval RMSE did not improve from the 0.5458 from epoch:2 step:20\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 220/567, Train_loss: 0.4755, Eval_loss:0.5512, Eval RMSE:0.5448\n",
      "Eval RMSE improved from 0.5458 to 0.5448\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 240/567, Train_loss: 0.4690, Eval_loss:0.5575, Eval RMSE:0.5509\n",
      "Eval RMSE did not improve from the 0.5448 from epoch:2 step:220\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 260/567, Train_loss: 0.4719, Eval_loss:0.5439, Eval RMSE:0.5348\n",
      "Eval RMSE improved from 0.5448 to 0.5348\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 280/567, Train_loss: 0.4694, Eval_loss:0.5606, Eval RMSE:0.5519\n",
      "Eval RMSE did not improve from the 0.5348 from epoch:2 step:260\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 300/567, Train_loss: 0.4686, Eval_loss:0.5432, Eval RMSE:0.5382\n",
      "Eval RMSE did not improve from the 0.5348 from epoch:2 step:260\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 320/567, Train_loss: 0.4700, Eval_loss:0.5488, Eval RMSE:0.5435\n",
      "Eval RMSE did not improve from the 0.5348 from epoch:2 step:260\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 340/567, Train_loss: 0.4670, Eval_loss:0.5458, Eval RMSE:0.5416\n",
      "Eval RMSE did not improve from the 0.5348 from epoch:2 step:260\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 360/567, Train_loss: 0.4671, Eval_loss:0.5426, Eval RMSE:0.5371\n",
      "Eval RMSE did not improve from the 0.5348 from epoch:2 step:260\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 380/567, Train_loss: 0.4641, Eval_loss:0.5406, Eval RMSE:0.5359\n",
      "Eval RMSE did not improve from the 0.5348 from epoch:2 step:260\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 400/567, Train_loss: 0.4638, Eval_loss:0.5314, Eval RMSE:0.5254\n",
      "Eval RMSE improved from 0.5348 to 0.5254\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 420/567, Train_loss: 0.4611, Eval_loss:0.5525, Eval RMSE:0.5497\n",
      "Eval RMSE did not improve from the 0.5254 from epoch:2 step:400\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 440/567, Train_loss: 0.4574, Eval_loss:0.5291, Eval RMSE:0.5271\n",
      "Eval RMSE did not improve from the 0.5254 from epoch:2 step:400\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 460/567, Train_loss: 0.4580, Eval_loss:0.5634, Eval RMSE:0.5610\n",
      "Eval RMSE did not improve from the 0.5254 from epoch:2 step:400\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 480/567, Train_loss: 0.4583, Eval_loss:0.5343, Eval RMSE:0.5310\n",
      "Eval RMSE did not improve from the 0.5254 from epoch:2 step:400\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 500/567, Train_loss: 0.4582, Eval_loss:0.5445, Eval RMSE:0.5415\n",
      "Eval RMSE did not improve from the 0.5254 from epoch:2 step:400\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 520/567, Train_loss: 0.4583, Eval_loss:0.5476, Eval RMSE:0.5458\n",
      "Eval RMSE did not improve from the 0.5254 from epoch:2 step:400\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 540/567, Train_loss: 0.4596, Eval_loss:0.5569, Eval RMSE:0.5550\n",
      "Eval RMSE did not improve from the 0.5254 from epoch:2 step:400\n",
      "\n",
      "Fold:5/5 Epoch:2/3 Step: 560/567, Train_loss: 0.4600, Eval_loss:0.5182, Eval RMSE:0.5147\n",
      "Eval RMSE improved from 0.5254 to 0.5147\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 5, Epoch: 2 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 0/567, Train_loss: 0.5399, Eval_loss:0.5310, Eval RMSE:0.5280\n",
      "Eval RMSE did not improve from the 0.5147 from epoch:2 step:560\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 20/567, Train_loss: 0.2900, Eval_loss:0.5372, Eval RMSE:0.5341\n",
      "Eval RMSE did not improve from the 0.5147 from epoch:2 step:560\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 40/567, Train_loss: 0.2985, Eval_loss:0.5210, Eval RMSE:0.5195\n",
      "Eval RMSE did not improve from the 0.5147 from epoch:2 step:560\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 60/567, Train_loss: 0.2805, Eval_loss:0.5155, Eval RMSE:0.5137\n",
      "Eval RMSE improved from 0.5147 to 0.5137\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 80/567, Train_loss: 0.2850, Eval_loss:0.5102, Eval RMSE:0.5077\n",
      "Eval RMSE improved from 0.5137 to 0.5077\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 100/567, Train_loss: 0.2786, Eval_loss:0.5093, Eval RMSE:0.5078\n",
      "Eval RMSE did not improve from the 0.5077 from epoch:3 step:80\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 120/567, Train_loss: 0.2787, Eval_loss:0.5160, Eval RMSE:0.5145\n",
      "Eval RMSE did not improve from the 0.5077 from epoch:3 step:80\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 140/567, Train_loss: 0.2802, Eval_loss:0.5097, Eval RMSE:0.5094\n",
      "Eval RMSE did not improve from the 0.5077 from epoch:3 step:80\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 160/567, Train_loss: 0.2787, Eval_loss:0.5086, Eval RMSE:0.5073\n",
      "Eval RMSE improved from 0.5077 to 0.5073\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 180/567, Train_loss: 0.2758, Eval_loss:0.5102, Eval RMSE:0.5079\n",
      "Eval RMSE did not improve from the 0.5073 from epoch:3 step:160\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 200/567, Train_loss: 0.2698, Eval_loss:0.5149, Eval RMSE:0.5131\n",
      "Eval RMSE did not improve from the 0.5073 from epoch:3 step:160\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 220/567, Train_loss: 0.2696, Eval_loss:0.5096, Eval RMSE:0.5074\n",
      "Eval RMSE did not improve from the 0.5073 from epoch:3 step:160\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 240/567, Train_loss: 0.2724, Eval_loss:0.5093, Eval RMSE:0.5068\n",
      "Eval RMSE improved from 0.5073 to 0.5068\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 260/567, Train_loss: 0.2697, Eval_loss:0.5063, Eval RMSE:0.5039\n",
      "Eval RMSE improved from 0.5068 to 0.5039\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 280/567, Train_loss: 0.2679, Eval_loss:0.5126, Eval RMSE:0.5100\n",
      "Eval RMSE did not improve from the 0.5039 from epoch:3 step:260\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 300/567, Train_loss: 0.2657, Eval_loss:0.5150, Eval RMSE:0.5119\n",
      "Eval RMSE did not improve from the 0.5039 from epoch:3 step:260\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 320/567, Train_loss: 0.2657, Eval_loss:0.5073, Eval RMSE:0.5042\n",
      "Eval RMSE did not improve from the 0.5039 from epoch:3 step:260\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 340/567, Train_loss: 0.2648, Eval_loss:0.5205, Eval RMSE:0.5180\n",
      "Eval RMSE did not improve from the 0.5039 from epoch:3 step:260\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 360/567, Train_loss: 0.2637, Eval_loss:0.5074, Eval RMSE:0.5041\n",
      "Eval RMSE did not improve from the 0.5039 from epoch:3 step:260\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 380/567, Train_loss: 0.2613, Eval_loss:0.5119, Eval RMSE:0.5080\n",
      "Eval RMSE did not improve from the 0.5039 from epoch:3 step:260\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 400/567, Train_loss: 0.2617, Eval_loss:0.5192, Eval RMSE:0.5158\n",
      "Eval RMSE did not improve from the 0.5039 from epoch:3 step:260\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 420/567, Train_loss: 0.2623, Eval_loss:0.5115, Eval RMSE:0.5074\n",
      "Eval RMSE did not improve from the 0.5039 from epoch:3 step:260\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 440/567, Train_loss: 0.2619, Eval_loss:0.5075, Eval RMSE:0.5036\n",
      "Eval RMSE improved from 0.5039 to 0.5036\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 460/567, Train_loss: 0.2596, Eval_loss:0.5077, Eval RMSE:0.5038\n",
      "Eval RMSE did not improve from the 0.5036 from epoch:3 step:440\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 480/567, Train_loss: 0.2577, Eval_loss:0.5068, Eval RMSE:0.5030\n",
      "Eval RMSE improved from 0.5036 to 0.5030\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 500/567, Train_loss: 0.2580, Eval_loss:0.5064, Eval RMSE:0.5025\n",
      "Eval RMSE improved from 0.5030 to 0.5025\n",
      "Saving the model ./fold_5_Roberta_large_model.bin\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 520/567, Train_loss: 0.2585, Eval_loss:0.5076, Eval RMSE:0.5039\n",
      "Eval RMSE did not improve from the 0.5025 from epoch:3 step:500\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 540/567, Train_loss: 0.2574, Eval_loss:0.5078, Eval RMSE:0.5039\n",
      "Eval RMSE did not improve from the 0.5025 from epoch:3 step:500\n",
      "\n",
      "Fold:5/5 Epoch:3/3 Step: 560/567, Train_loss: 0.2582, Eval_loss:0.5076, Eval RMSE:0.5038\n",
      "Eval RMSE did not improve from the 0.5025 from epoch:3 step:500\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "----------------------Fold: 5, Epoch: 3 over----------------------\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold 5/5 Best Eval RMSE:0.502530574798584, Best epoch:3, Best step:500\n",
      "\n",
      "#######################################################################\n",
      "Fold summary\n",
      "{'fold_1': {'best_epoch': 1, 'best_step': 200, 'best_score': 0.5923193}, 'fold_2': {'best_epoch': 3, 'best_step': 160, 'best_score': 0.48741987}, 'fold_3': {'best_epoch': 1, 'best_step': 300, 'best_score': 0.5497123}, 'fold_4': {'best_epoch': 3, 'best_step': 480, 'best_score': 0.49744383}, 'fold_5': {'best_epoch': 3, 'best_step': 500, 'best_score': 0.5025306}}\n"
     ]
    }
   ],
   "source": [
    "use_fold = 'tb_fold'\n",
    "\n",
    "total_fold = train_df[use_fold].nunique()\n",
    "\n",
    "params_dict = {'Max_length':256,\n",
    "               'train_batch_size':4,\n",
    "               'valid_batch_size':128,\n",
    "               'learning_rate':4e-5,\n",
    "               'EPS':3e-8, \n",
    "               'weight_dict':0, \n",
    "               'opt': 'ADAMW', # MADGRAD, ADAM, ADAMW\n",
    "               'scheduler':False,\n",
    "               'epoch':3,\n",
    "               'eval_every_step':20,\n",
    "               'total_folds':total_fold\n",
    "              }\n",
    "\n",
    "class ContinuousStratifiedKFold(StratifiedKFold):\n",
    "    def split(selfself, x, y, groups=None):\n",
    "        num_bins = int(np.floor(1 + np.log2(len(y))))\n",
    "        bins = pd.cut(y, bins=num_bins, labels=False)\n",
    "        return super().split(x, bins, groups)\n",
    "    \n",
    "gc.collect()\n",
    "\n",
    "folds_best = {}\n",
    "SEED = 42\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "# kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
    "# for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
    "    \n",
    "\n",
    "kfold = ContinuousStratifiedKFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df, train_df.target)):    \n",
    "\n",
    "    fold = fold+1\n",
    "    \n",
    "    print(f\"\\nFold {fold}/{NUM_FOLDS}\")\n",
    "    model_path = f\"model_large_{fold}.pth\"\n",
    "        \n",
    "    set_random_seed(SEED + fold)\n",
    "    \n",
    "    train_fold = train_df.loc[train_indices].reset_index(drop=True)\n",
    "    validation_fold = train_df.loc[val_indices].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "# for i in range(1, total_fold+1):\n",
    "    \n",
    "#     fold_rmse = []\n",
    "#     train_fold = train_df[train_df[use_fold]!=i].reset_index(drop=True)\n",
    "#     validation_fold = train_df[train_df[use_fold]==i].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Fold {fold}/{total_fold}: Train fold: {train_fold[use_fold].unique()}, Test fold:{validation_fold[use_fold].unique()}\")\n",
    "\n",
    "    train_fold_input = get_input(df=train_fold, data_type='train')\n",
    "    validation_fold_input = get_input(df=validation_fold, data_type='train')\n",
    "\n",
    "    train_data_loader = get_data_loader(class_input=train_fold_input, \n",
    "                                        batch_size=params_dict['train_batch_size'], \n",
    "                                        shuffle=True)\n",
    "    valid_data_loader = get_data_loader(class_input=validation_fold_input, \n",
    "                                        batch_size=params_dict['valid_batch_size'], \n",
    "                                        shuffle=False)\n",
    "\n",
    "\n",
    "    best_score, best_epoch, best_step = train_engine(train_data_loader=train_data_loader, \n",
    "                                                     eval_data_loader=valid_data_loader, \n",
    "                                                     fold_no=fold)\n",
    "    \n",
    "    folds_best[f'fold_{fold}'] = {}\n",
    "    folds_best[f'fold_{fold}'] = {'best_epoch': best_epoch, \n",
    "                                  'best_step':best_step,\n",
    "                                  'best_score':best_score\n",
    "                              }\n",
    "    \n",
    "    print(f\"Fold {fold}/{total_fold} Best Eval RMSE:{best_score}, Best epoch:{best_epoch}, Best step:{best_step}\")\n",
    "    print(\"\")\n",
    "    print(\"#######################################################################\")\n",
    "    \n",
    "print(\"Fold summary\")\n",
    "print(folds_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T15:17:31.227010Z",
     "iopub.status.busy": "2021-07-23T15:17:31.226413Z",
     "iopub.status.idle": "2021-07-23T15:17:31.230775Z",
     "shell.execute_reply": "2021-07-23T15:17:31.231236Z"
    },
    "papermill": {
     "duration": 0.165385,
     "end_time": "2021-07-23T15:17:31.231376",
     "exception": false,
     "start_time": "2021-07-23T15:17:31.065991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold_1': {'best_epoch': 1, 'best_step': 200, 'best_score': 0.5923193},\n",
       " 'fold_2': {'best_epoch': 3, 'best_step': 160, 'best_score': 0.48741987},\n",
       " 'fold_3': {'best_epoch': 1, 'best_step': 300, 'best_score': 0.5497123},\n",
       " 'fold_4': {'best_epoch': 3, 'best_step': 480, 'best_score': 0.49744383},\n",
       " 'fold_5': {'best_epoch': 3, 'best_step': 500, 'best_score': 0.5025306}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-23T15:17:31.542430Z",
     "iopub.status.busy": "2021-07-23T15:17:31.541670Z",
     "iopub.status.idle": "2021-07-23T15:17:31.544785Z",
     "shell.execute_reply": "2021-07-23T15:17:31.545289Z",
     "shell.execute_reply.started": "2021-07-23T09:34:13.439111Z"
    },
    "papermill": {
     "duration": 0.161223,
     "end_time": "2021-07-23T15:17:31.545432",
     "exception": false,
     "start_time": "2021-07-23T15:17:31.384209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fold_1': {'best_epoch': 1, 'best_step': 200, 'best_score': 0.5923193},\n",
       " 'fold_2': {'best_epoch': 3, 'best_step': 160, 'best_score': 0.48741987},\n",
       " 'fold_3': {'best_epoch': 1, 'best_step': 300, 'best_score': 0.5497123},\n",
       " 'fold_4': {'best_epoch': 3, 'best_step': 480, 'best_score': 0.49744383},\n",
       " 'fold_5': {'best_epoch': 3, 'best_step': 500, 'best_score': 0.5025306}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.153695,
     "end_time": "2021-07-23T15:17:31.851850",
     "exception": false,
     "start_time": "2021-07-23T15:17:31.698155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.153283,
     "end_time": "2021-07-23T15:17:32.162245",
     "exception": false,
     "start_time": "2021-07-23T15:17:32.008962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12330.70436,
   "end_time": "2021-07-23T15:17:35.940096",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-23T11:52:05.235736",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
